{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Belle2 MonteCarlo Data\n",
    "\n",
    "__Authors:__\n",
    "- Valeria\n",
    "- Matteo\n",
    "- Philipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pprint\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define branches that we need for our analysis\n",
    "Masses = ['B0_M', 'B0_ErrM', 'B0_SigM', 'B0_K_S0_M', 'B0_K_S0_ErrM', 'B0_K_S0_SigM', 'B0_etap_M', 'B0_etap_ErrM', \n",
    "          'B0_etap_SigM', 'B0_etap_eta_M', 'B0_etap_eta_ErrM', 'B0_etap_eta_SigM']\n",
    "Kinetics_CMS = ['B0_Pcms', 'B0_etap_Pcms', 'B0_etap_eta_Pcms', 'B0_etap_eta_gamma0_Pcms', 'B0_etap_eta_gamma1_Pcms',\n",
    "                'B0_etap_pi0_Pcms', 'B0_etap_pi1_Pcms', 'B0_K_S0_Pcms']\n",
    "Other_Kinetics = ['B0_deltae', 'B0_mbc']\n",
    "DecayAngles = ['B0_decayAngle__bo0__bc', 'B0_decayAngle__bo1__bc', 'B0_etap_decayAngle__bo0__bc',\n",
    "               'B0_etap_decayAngle__bo1__bc', 'B0_etap_decayAngle__bo2__bc']\n",
    "Positions = ['B0_X', 'B0_ErrX', 'B0_Y', 'B0_ErrY', 'B0_Z', 'B0_ErrZ', 'B0_Rho',  \n",
    "             'B0_etap_X', 'B0_etap_ErrX', 'B0_etap_Y', 'B0_etap_ErrY', \n",
    "             'B0_etap_Z', 'B0_etap_ErrZ', 'B0_etap_Rho',\n",
    "             'B0_etap_eta_X', 'B0_etap_eta_ErrX', 'B0_etap_eta_Y',\n",
    "             'B0_etap_eta_ErrY', 'B0_etap_eta_Z', 'B0_etap_eta_ErrZ', 'B0_etap_eta_Rho',\n",
    "             'B0_etap_pi0_X', 'B0_etap_pi0_ErrX', 'B0_etap_pi0_Y', 'B0_etap_pi0_ErrY', \n",
    "             'B0_etap_pi0_Z', 'B0_etap_pi0_ErrZ', 'B0_etap_pi0_Rho', \n",
    "             'B0_etap_pi1_X', 'B0_etap_pi1_ErrX', 'B0_etap_pi1_Y', 'B0_etap_pi1_ErrY', \n",
    "             'B0_etap_pi1_Z', 'B0_etap_pi1_ErrZ', 'B0_etap_pi1_Rho', \n",
    "             'B0_K_S0_X', 'B0_K_S0_ErrX', 'B0_K_S0_Y', 'B0_K_S0_ErrY', 'B0_K_S0_Z',\n",
    "             'B0_K_S0_ErrZ', 'B0_K_S0_Rho', \n",
    "             'B0_cosAngleBetweenMomentumAndVertexVector', 'B0_distance', 'B0_significanceOfDistance',\n",
    "             'B0_dr', 'B0_etap_pi0_dr', 'B0_etap_pi1_dr', 'B0_K_S0_dr']\n",
    "Vertex_Training = ['B0_VtxPvalue', 'B0_etap_VtxPvalue', 'B0_etap_eta_VtxPvalue', 'B0_etap_pi0_VtxPvalue',\n",
    "                   'B0_etap_pi1_VtxPvalue', 'B0_K_S0_VtxPvalue', ]\n",
    "Continuum_Suppression_Training = ['B0_TrCSMVA']\n",
    "\n",
    "Training = Kinetics_CMS + Masses + Other_Kinetics + Continuum_Suppression_Training + Positions + DecayAngles + Vertex_Training\n",
    "Important = Training + ['B0_isSignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.16/00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/root_numpy/__init__.py:46: RuntimeWarning: numpy 1.16.4 is currently installed but you installed root_numpy against numpy 1.9.3. Please consider reinstalling root_numpy for this numpy version.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from root_pandas import read_root\n",
    "\n",
    "path = '/home/philipp/Desktop/Project/DATA/'\n",
    "SFiles = glob.glob(os.path.join(path, 'Signal/*.root'))\n",
    "CFiles = glob.glob(os.path.join(path, 'Continuous/*.root'))\n",
    "PFiles = glob.glob(os.path.join(path, 'Peaking/*.root'))\n",
    "\n",
    "Full_Signal = pd.concat((read_root(f, 'B0', columns=Important) for f in SFiles))\n",
    "Full_Signal = Full_Signal[Full_Signal['B0_isSignal']==1].reset_index(drop=True)\n",
    "Full_Continuous = pd.concat((read_root(f, 'B0', columns=Important) for f in CFiles))\n",
    "Full_Peaking = pd.concat((read_root(f, 'B0', columns=Important) for f in PFiles))\n",
    "\n",
    "Signal = Full_Signal[Training]\n",
    "Continuous = Full_Continuous[Training]\n",
    "Peaking = Full_Peaking[Training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal['Type'] = 2\n",
    "Continuous['Type'] = 1\n",
    "Peaking['Type'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "n_seed=1234\n",
    "seed(n_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Sum_BS = pd.concat([Signal, Continuous, Peaking]).sample(frac=1)\n",
    "X = Sum_BS.drop('Type',axis=1)\n",
    "Y = Sum_BS['Type']\n",
    "\n",
    "# For Testing purposes it's convenient to work with a small subset\n",
    "if Testing : \n",
    "    N_Events = 10000\n",
    "    X = X[:N_Events]\n",
    "    Y = Y[:N_Events]\n",
    "\n",
    "# We split our dataset into 70% training, 15% testing and 15% validation \n",
    "X_Train, X_Rest, Y_Train, Y_Rest = train_test_split(X, Y, train_size=0.7,random_state=randint(10**6,10**9))\n",
    "X_Test, X_Validation, Y_Test, Y_Validation = train_test_split(X_Rest, Y_Rest, train_size=0.5,random_state=randint(10**6,10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_Train = ss.fit_transform(X_Train)\n",
    "X_Test = ss.transform(X_Test)\n",
    "X_Validation = ss.transform(X_Validation)\n",
    "\n",
    "Y_Train = to_categorical(Y_Train, num_classes=3) \n",
    "Y_Test = to_categorical(Y_Test, num_classes=3)\n",
    "Y_Validation = to_categorical(Y_Validation, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 variables explain 50% of the variance\n",
      "35 variables explain 90% of the variance\n",
      "41 variables explain 95% of the variance\n",
      "50 variables explain 99% of the variance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=len(Training))\n",
    "pcTrain = pd.DataFrame(data = pca.fit_transform(X_Train))\n",
    "pcTest = pd.DataFrame(data = pca.transform(X_Test))\n",
    "pcValidation = pd.DataFrame(data = pca.transform(X_Validation))\n",
    "\n",
    "T50, T90, T95, T99, = False, False, False, False\n",
    "for i in range(len(pca.explained_variance_ratio_)) : \n",
    "    if (sum(pca.explained_variance_ratio_[:i+1]) > 0.5 and T50 ==False) : \n",
    "        print(str(i+1) + ' variables explain 50% of the variance')\n",
    "        T50 = True\n",
    "        n_50 = i + 1 \n",
    "    if (sum(pca.explained_variance_ratio_[:i+1]) > 0.9 and T90 ==False) : \n",
    "        print(str(i+1) + ' variables explain 90% of the variance')\n",
    "        T90 = True\n",
    "        n_90 = i + 1\n",
    "    if (sum(pca.explained_variance_ratio_[:i+1]) > 0.95 and T95 ==False) : \n",
    "        print(str(i+1) + ' variables explain 95% of the variance')\n",
    "        T95 = True\n",
    "        n_95 = i + 1\n",
    "    if (sum(pca.explained_variance_ratio_[:i+1]) > 0.99 and T99 ==False) : \n",
    "        print(str(i+1) + ' variables explain 99% of the variance')\n",
    "        T99 = True\n",
    "        n_99 = i + 1\n",
    "        \n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For input_dim chose n_50, n_90, n_95 or n_99 depending on how much explained variance we require\n",
    "input_dim = n_99\n",
    "pcTrain = pcTrain.iloc[:, 0:input_dim]\n",
    "pcTest = pcTest.iloc[:, 0:input_dim]\n",
    "pcValidation = pcValidation.iloc[:, 0:input_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ['SGD', 'Adam']\n",
    "epochs = [2, 5]\n",
    "batch_size = [100, 1000, 10000]\n",
    "architectures = [ [10, 10], [20, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def build_DNN():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(pcTrain.shape[1],)))\n",
    "    model.add(Dense(layers[0], input_shape=(pcTrain.shape[1],), activation='relu'))\n",
    "    for i in range(1,len(layers)):\n",
    "        model.add(Dense(layers[i], activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def print_results() :\n",
    "    print(\"Layers: \", layers)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params): \n",
    "        print(\"%f (%f) with %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as the GridSearch does not provide the possibility to try out different architectures. \n",
    "As a workaround I suggest defining layers=[xx, xx, xx] for a couple of different architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using architecture:  [10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 13:49:09.015285 140477963200320 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 13:49:09.030999 140477963200320 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 13:49:09.033092 140477963200320 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 13:49:09.044497 140477963200320 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 13:49:09.056702 140477963200320 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 13:49:09.148205 140477963200320 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 13:49:09.169569 140477963200320 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 13:49:09.240481 140477963200320 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.9251 - acc: 0.6150\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 0s 14us/step - loss: 0.6897 - acc: 0.7744\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 0s 15us/step - loss: 0.4730 - acc: 0.8704\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 0s 14us/step - loss: 0.3149 - acc: 0.9054\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 0s 15us/step - loss: 0.2619 - acc: 0.9104\n",
      "Layers:  [10, 10]\n",
      "Best: 0.821571 using {'batch_size': 100, 'epochs': 5}\n",
      "0.649571 (0.037387) with {'batch_size': 100, 'epochs': 2}\n",
      "0.821571 (0.089704) with {'batch_size': 100, 'epochs': 5}\n",
      "0.336429 (0.215050) with {'batch_size': 1000, 'epochs': 2}\n",
      "0.401857 (0.173829) with {'batch_size': 1000, 'epochs': 5}\n",
      "0.328857 (0.103412) with {'batch_size': 10000, 'epochs': 2}\n",
      "0.377571 (0.157619) with {'batch_size': 10000, 'epochs': 5}\n",
      "Using architecture:  [20, 20]\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 1s 123us/step - loss: 0.9825 - acc: 0.5381\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 18us/step - loss: 0.6903 - acc: 0.7668\n",
      "2334/2334 [==============================] - 0s 41us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 129us/step - loss: 1.1351 - acc: 0.3908\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 18us/step - loss: 0.7734 - acc: 0.7013\n",
      "2333/2333 [==============================] - 0s 45us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 133us/step - loss: 0.9958 - acc: 0.5243\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 18us/step - loss: 0.7402 - acc: 0.6368\n",
      "2333/2333 [==============================] - 0s 51us/step\n",
      "Epoch 1/5\n",
      "4666/4666 [==============================] - 1s 140us/step - loss: 0.9199 - acc: 0.5186\n",
      "Epoch 2/5\n",
      "4666/4666 [==============================] - 0s 20us/step - loss: 0.6698 - acc: 0.6545\n",
      "Epoch 3/5\n",
      "4666/4666 [==============================] - 0s 20us/step - loss: 0.4851 - acc: 0.8285\n",
      "Epoch 4/5\n",
      "4666/4666 [==============================] - 0s 17us/step - loss: 0.3331 - acc: 0.8845\n",
      "Epoch 5/5\n",
      "4666/4666 [==============================] - 0s 16us/step - loss: 0.2625 - acc: 0.9051\n",
      "2334/2334 [==============================] - 0s 54us/step\n",
      "Epoch 1/5\n",
      "4667/4667 [==============================] - 1s 147us/step - loss: 0.8703 - acc: 0.6218\n",
      "Epoch 2/5\n",
      "4667/4667 [==============================] - 0s 18us/step - loss: 0.6206 - acc: 0.7735\n",
      "Epoch 3/5\n",
      "4667/4667 [==============================] - 0s 19us/step - loss: 0.4244 - acc: 0.8569\n",
      "Epoch 4/5\n",
      "4667/4667 [==============================] - 0s 16us/step - loss: 0.2899 - acc: 0.8974\n",
      "Epoch 5/5\n",
      "4667/4667 [==============================] - 0s 16us/step - loss: 0.2400 - acc: 0.9128\n",
      "2333/2333 [==============================] - 0s 61us/step\n",
      "Epoch 1/5\n",
      "4667/4667 [==============================] - 1s 151us/step - loss: 1.1611 - acc: 0.3735\n",
      "Epoch 2/5\n",
      "4667/4667 [==============================] - 0s 17us/step - loss: 0.7931 - acc: 0.7326\n",
      "Epoch 3/5\n",
      "4667/4667 [==============================] - 0s 16us/step - loss: 0.5247 - acc: 0.8374\n",
      "Epoch 4/5\n",
      "4667/4667 [==============================] - 0s 16us/step - loss: 0.3367 - acc: 0.8839\n",
      "Epoch 5/5\n",
      "4667/4667 [==============================] - 0s 18us/step - loss: 0.2573 - acc: 0.9100\n",
      "2333/2333 [==============================] - 0s 63us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 1s 146us/step - loss: 1.1529 - acc: 0.3133\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 3us/step - loss: 1.0921 - acc: 0.3922\n",
      "2334/2334 [==============================] - 0s 67us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 152us/step - loss: 1.2513 - acc: 0.4307\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 1.1703 - acc: 0.4483\n",
      "2333/2333 [==============================] - 0s 74us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 158us/step - loss: 1.0010 - acc: 0.4168\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 0.9480 - acc: 0.4570\n",
      "2333/2333 [==============================] - 0s 77us/step\n",
      "Epoch 1/5\n",
      "4666/4666 [==============================] - 1s 166us/step - loss: 1.4310 - acc: 0.0649\n",
      "Epoch 2/5\n",
      "4666/4666 [==============================] - 0s 3us/step - loss: 1.3528 - acc: 0.1067\n",
      "Epoch 3/5\n",
      "4666/4666 [==============================] - 0s 4us/step - loss: 1.2749 - acc: 0.1607\n",
      "Epoch 4/5\n",
      "4666/4666 [==============================] - 0s 4us/step - loss: 1.1965 - acc: 0.2342\n",
      "Epoch 5/5\n",
      "4666/4666 [==============================] - 0s 4us/step - loss: 1.1363 - acc: 0.3127\n",
      "2334/2334 [==============================] - 0s 84us/step\n",
      "Epoch 1/5\n",
      "4667/4667 [==============================] - 1s 174us/step - loss: 1.0093 - acc: 0.5183\n",
      "Epoch 2/5\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 0.9659 - acc: 0.5271\n",
      "Epoch 3/5\n",
      "4667/4667 [==============================] - 0s 5us/step - loss: 0.9259 - acc: 0.5301\n",
      "Epoch 4/5\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 0.8912 - acc: 0.5391\n",
      "Epoch 5/5\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 0.8542 - acc: 0.5552\n",
      "2333/2333 [==============================] - 0s 89us/step\n",
      "Epoch 1/5\n",
      "4667/4667 [==============================] - 1s 177us/step - loss: 1.0092 - acc: 0.4596\n",
      "Epoch 2/5\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 0.9744 - acc: 0.5065\n",
      "Epoch 3/5\n",
      "4667/4667 [==============================] - 0s 5us/step - loss: 0.9480 - acc: 0.5245\n",
      "Epoch 4/5\n",
      "4667/4667 [==============================] - 0s 5us/step - loss: 0.9177 - acc: 0.5659\n",
      "Epoch 5/5\n",
      "4667/4667 [==============================] - 0s 6us/step - loss: 0.8878 - acc: 0.5873\n",
      "2333/2333 [==============================] - 0s 93us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 1s 183us/step - loss: 1.0260 - acc: 0.5041\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 2us/step - loss: 1.0173 - acc: 0.5152\n",
      "2334/2334 [==============================] - 0s 99us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 186us/step - loss: 1.3064 - acc: 0.2959\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 1.2892 - acc: 0.3064\n",
      "2333/2333 [==============================] - 0s 102us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 191us/step - loss: 1.1996 - acc: 0.2488\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 1.1836 - acc: 0.2661\n",
      "2333/2333 [==============================] - 0s 109us/step\n",
      "Epoch 1/5\n",
      "4666/4666 [==============================] - 1s 201us/step - loss: 1.1424 - acc: 0.2473\n",
      "Epoch 2/5\n",
      "4666/4666 [==============================] - 0s 3us/step - loss: 1.1303 - acc: 0.2610\n",
      "Epoch 3/5\n",
      "4666/4666 [==============================] - 0s 1us/step - loss: 1.1181 - acc: 0.2795\n",
      "Epoch 4/5\n",
      "4666/4666 [==============================] - 0s 1us/step - loss: 1.1101 - acc: 0.2983\n",
      "Epoch 5/5\n",
      "4666/4666 [==============================] - 0s 1us/step - loss: 1.1023 - acc: 0.3080\n",
      "2334/2334 [==============================] - 0s 115us/step\n",
      "Epoch 1/5\n",
      "4667/4667 [==============================] - 1s 207us/step - loss: 1.0386 - acc: 0.4155\n",
      "Epoch 2/5\n",
      "4667/4667 [==============================] - 0s 2us/step - loss: 1.0308 - acc: 0.4202\n",
      "Epoch 3/5\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.0233 - acc: 0.4243\n",
      "Epoch 4/5\n",
      "4667/4667 [==============================] - 0s 2us/step - loss: 1.0174 - acc: 0.4371\n",
      "Epoch 5/5\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.0065 - acc: 0.4489\n",
      "2333/2333 [==============================] - 0s 122us/step\n",
      "Epoch 1/5\n",
      "4667/4667 [==============================] - 1s 219us/step - loss: 1.0042 - acc: 0.4489\n",
      "Epoch 2/5\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 0.9915 - acc: 0.4598\n",
      "Epoch 3/5\n",
      "4667/4667 [==============================] - 0s 2us/step - loss: 0.9895 - acc: 0.4663\n",
      "Epoch 4/5\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 0.9778 - acc: 0.4780\n",
      "Epoch 5/5\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 0.9662 - acc: 0.4883\n",
      "2333/2333 [==============================] - 0s 125us/step\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 166us/step - loss: 0.9187 - acc: 0.5903\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 20us/step - loss: 0.4965 - acc: 0.8483\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 0s 18us/step - loss: 0.2801 - acc: 0.9050\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 0s 17us/step - loss: 0.2235 - acc: 0.9199\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 0s 18us/step - loss: 0.2150 - acc: 0.9209\n",
      "Layers:  [20, 20]\n",
      "Best: 0.958857 using {'batch_size': 100, 'epochs': 5}\n",
      "0.802571 (0.063420) with {'batch_size': 100, 'epochs': 2}\n",
      "0.958857 (0.003558) with {'batch_size': 100, 'epochs': 5}\n",
      "0.478000 (0.011174) with {'batch_size': 1000, 'epochs': 2}\n",
      "0.522857 (0.084294) with {'batch_size': 1000, 'epochs': 5}\n",
      "0.379286 (0.113393) with {'batch_size': 10000, 'epochs': 2}\n",
      "0.434000 (0.071010) with {'batch_size': 10000, 'epochs': 5}\n",
      "Epoch 1/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 1:30 - loss: 1.1570 - acc: 0.3400Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:30 - loss: 1.1614 - acc: 0.3000\n",
      "3200/4667 [===================>..........] - ETA: 0s - loss: 1.1178 - acc: 0.3381  \n",
      "1700/4666 [=========>....................] - ETA: 3s - loss: 1.0849 - acc: 0.4300  Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:30 - loss: 0.9624 - acc: 0.5500\n",
      "4667/4667 [==============================] - 2s 442us/step - loss: 1.0914 - acc: 0.3917\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 1.0149 - acc: 0.4800\n",
      "4000/4666 [========================>.....] - ETA: 0s - loss: 1.0242 - acc: 0.4497\n",
      "2100/4667 [============>.................] - ETA: 2s - loss: 0.9459 - acc: 0.5148  \n",
      "4666/4666 [==============================] - 2s 449us/step - loss: 1.0082 - acc: 0.4638\n",
      "Epoch 1/5\n",
      "\n",
      " 100/4666 [..............................] - ETA: 1:27 - loss: 1.3546 - acc: 0.2600Epoch 2/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 0.9164 - acc: 0.5300\n",
      "2400/4667 [==============>...............] - ETA: 0s - loss: 0.9920 - acc: 0.5633\n",
      "4200/4667 [=========================>....] - ETA: 0s - loss: 0.9131 - acc: 0.5321\n",
      "2200/4666 [=============>................] - ETA: 0s - loss: 0.8793 - acc: 0.5414\n",
      "2200/4666 [=============>................] - ETA: 2s - loss: 1.2437 - acc: 0.3100  \n",
      "4667/4667 [==============================] - 2s 451us/step - loss: 0.9068 - acc: 0.5327\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.8138 - acc: 0.6000\n",
      "4500/4667 [===========================>..] - ETA: 0s - loss: 0.9621 - acc: 0.5924\n",
      "4667/4667 [==============================] - 0s 24us/step - loss: 0.9600 - acc: 0.5940\n",
      "\n",
      "4100/4666 [=========================>....] - ETA: 0s - loss: 0.8520 - acc: 0.5505\n",
      "4300/4666 [==========================>...] - ETA: 0s - loss: 1.1762 - acc: 0.3586\n",
      "4666/4666 [==============================] - 2s 434us/step - loss: 1.1656 - acc: 0.3650\n",
      "\n",
      "2100/4667 [============>.................] - ETA: 0s - loss: 0.8133 - acc: 0.5690\n",
      "4666/4666 [==============================] - 0s 26us/step - loss: 0.8455 - acc: 0.5572\n",
      "Epoch 2/5\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 1.0601 - acc: 0.4000\n",
      "4300/4667 [==========================>...] - ETA: 0s - loss: 0.7907 - acc: 0.6040\n",
      "4667/4667 [==============================] - 0s 24us/step - loss: 0.7879 - acc: 0.6064\n",
      "\n",
      "2600/4666 [===============>..............] - ETA: 0s - loss: 0.9625 - acc: 0.4773\n",
      "4666/4666 [==============================] - 0s 23us/step - loss: 0.9262 - acc: 0.4976\n",
      "Epoch 3/5\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 0.8518 - acc: 0.5000\n",
      "2900/4666 [=================>............] - ETA: 0s - loss: 0.8046 - acc: 0.5579\n",
      "4666/4666 [==============================] - 0s 19us/step - loss: 0.7938 - acc: 0.5630\n",
      "Epoch 4/5\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 0.8065 - acc: 0.4600\n",
      "2800/4666 [=================>............] - ETA: 0s - loss: 0.7238 - acc: 0.6218\n",
      "4666/4666 [==============================] - 0s 18us/step - loss: 0.7139 - acc: 0.6299\n",
      "Epoch 5/5\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 0.7075 - acc: 0.6000\n",
      "2900/4666 [=================>............] - ETA: 0s - loss: 0.6726 - acc: 0.6841\n",
      " 100/2333 [>.............................] - ETA: 8s\n",
      "4666/4666 [==============================] - 0s 17us/step - loss: 0.6593 - acc: 0.6963\n",
      "\n",
      "2333/2333 [==============================] - 0s 184us/step\n",
      "\n",
      " 100/2334 [>.............................] - ETA: 8s\n",
      " 100/2333 [>.............................] - ETA: 7s\n",
      "2334/2334 [==============================] - 0s 178us/step\n",
      "\n",
      "2333/2333 [==============================] - 0s 167us/step\n",
      "\n",
      " 100/2334 [>.............................] - ETA: 7s\n",
      "2334/2334 [==============================] - 0s 163us/step\n",
      "Epoch 1/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:34 - loss: 1.1829 - acc: 0.2400Epoch 1/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:34 - loss: 1.5423 - acc: 0.1700Epoch 1/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 7s - loss: 1.2511 - acc: 0.1970\n",
      "4666/4666 [==============================] - 2s 444us/step - loss: 1.2512 - acc: 0.2184\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 1.2136 - acc: 0.2290\n",
      "2300/4667 [=============>................] - ETA: 2s - loss: 1.1258 - acc: 0.4030  \n",
      "1900/4667 [===========>..................] - ETA: 3s - loss: 1.5166 - acc: 0.1532  \n",
      "4666/4666 [==============================] - 0s 5us/step - loss: 1.2161 - acc: 0.2400\n",
      "\n",
      "4500/4667 [===========================>..] - ETA: 0s - loss: 1.0735 - acc: 0.4769\n",
      "4667/4667 [==============================] - 2s 466us/step - loss: 1.0698 - acc: 0.4806\n",
      "Epoch 2/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.9528 - acc: 0.5800\n",
      "4000/4667 [========================>.....] - ETA: 0s - loss: 1.4199 - acc: 0.1943\n",
      "4667/4667 [==============================] - 2s 467us/step - loss: 1.3929 - acc: 0.2115\n",
      "Epoch 2/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 1.2059 - acc: 0.3600\n",
      "2200/4667 [=============>................] - ETA: 0s - loss: 0.9333 - acc: 0.6023\n",
      "2100/4667 [============>.................] - ETA: 0s - loss: 1.1828 - acc: 0.3781\n",
      "4500/4667 [===========================>..] - ETA: 0s - loss: 0.8909 - acc: 0.6269\n",
      "4667/4667 [==============================] - 0s 24us/step - loss: 0.8871 - acc: 0.6285\n",
      "Epoch 3/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.8126 - acc: 0.6800\n",
      "4667/4667 [==============================] - 0s 24us/step - loss: 1.1240 - acc: 0.4247\n",
      "Epoch 3/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 1.0716 - acc: 0.4900\n",
      "2200/4667 [=============>................] - ETA: 0s - loss: 0.7577 - acc: 0.6941\n",
      "2600/4667 [===============>..............] - ETA: 0s - loss: 0.9810 - acc: 0.5031\n",
      "4500/4667 [===========================>..] - ETA: 0s - loss: 0.7144 - acc: 0.7233\n",
      "4667/4667 [==============================] - 0s 24us/step - loss: 0.7114 - acc: 0.7257\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 7s - loss: 1.3056 - acc: 0.0460Epoch 4/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.5812 - acc: 0.8200\n",
      "4667/4667 [==============================] - 2s 446us/step - loss: 1.2751 - acc: 0.0636\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.2609 - acc: 0.0840\n",
      "4667/4667 [==============================] - 0s 24us/step - loss: 0.9468 - acc: 0.5067\n",
      "Epoch 4/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.8965 - acc: 0.4800\n",
      "4667/4667 [==============================] - 0s 5us/step - loss: 1.2468 - acc: 0.0874\n",
      "\n",
      "2000/4667 [===========>..................] - ETA: 0s - loss: 0.5828 - acc: 0.8005\n",
      "2300/4667 [=============>................] - ETA: 0s - loss: 0.8385 - acc: 0.5257\n",
      "1000/2334 [===========>..................] - ETA: 0s\n",
      "2334/2334 [==============================] - 0s 154us/step\n",
      "\n",
      "4600/4667 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.8241\n",
      "4667/4667 [==============================] - 0s 25us/step - loss: 0.5385 - acc: 0.8254\n",
      "Epoch 5/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.4430 - acc: 0.8800\n",
      "4667/4667 [==============================] - 0s 24us/step - loss: 0.8069 - acc: 0.5395\n",
      "Epoch 5/5\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.7714 - acc: 0.5800\n",
      "2300/4667 [=============>................] - ETA: 0s - loss: 0.4195 - acc: 0.8770\n",
      "2500/4667 [===============>..............] - ETA: 0s - loss: 0.7254 - acc: 0.6168\n",
      "4300/4667 [==========================>...] - ETA: 0s - loss: 0.4064 - acc: 0.8805\n",
      "4667/4667 [==============================] - 0s 24us/step - loss: 0.4019 - acc: 0.8817\n",
      "\n",
      "4300/4667 [==========================>...] - ETA: 0s - loss: 0.7137 - acc: 0.6360\n",
      "4667/4667 [==============================] - 0s 26us/step - loss: 0.7104 - acc: 0.6392\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 0s 164us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 7s\n",
      " 100/2333 [>.............................] - ETA: 7s\n",
      "2333/2333 [==============================] - 0s 160us/step\n",
      "\n",
      "2333/2333 [==============================] - 0s 160us/step\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 7s - loss: 0.9134 - acc: 0.5600\n",
      "4667/4667 [==============================] - 2s 468us/step - loss: 0.8910 - acc: 0.5920\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 0.8699 - acc: 0.6230\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 0.8689 - acc: 0.6165\n",
      "Epoch 1/5\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 7s - loss: 1.2781 - acc: 0.1480\n",
      "4666/4666 [==============================] - 2s 461us/step - loss: 1.2681 - acc: 0.1682\n",
      "Epoch 2/5\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 1.2596 - acc: 0.1650\n",
      "4666/4666 [==============================] - 0s 3us/step - loss: 1.2461 - acc: 0.1841\n",
      "Epoch 3/5\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 1.2343 - acc: 0.2210\n",
      "4666/4666 [==============================] - 0s 5us/step - loss: 1.2210 - acc: 0.2167\n",
      "Epoch 4/5\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 1.1945 - acc: 0.2170\n",
      "4666/4666 [==============================] - 0s 2us/step - loss: 1.1941 - acc: 0.2302\n",
      "Epoch 5/5\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 1.1854 - acc: 0.2350\n",
      "4666/4666 [==============================] - 0s 4us/step - loss: 1.1736 - acc: 0.2593\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 0s 173us/step\n",
      "Epoch 1/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 7s - loss: 1.4087 - acc: 0.1000Epoch 1/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 8s - loss: 0.9829 - acc: 0.5300\n",
      "4667/4667 [==============================] - 2s 470us/step - loss: 1.3914 - acc: 0.1121\n",
      "Epoch 2/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.3758 - acc: 0.1390\n",
      "4667/4667 [==============================] - 2s 474us/step - loss: 0.9552 - acc: 0.5466\n",
      "Epoch 2/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 0.9452 - acc: 0.5750\n",
      "4667/4667 [==============================] - 0s 5us/step - loss: 1.3339 - acc: 0.1442\n",
      "\n",
      "4667/4667 [==============================] - 0s 6us/step - loss: 0.9372 - acc: 0.5742\n",
      "Epoch 3/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.3073 - acc: 0.1660Epoch 3/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 0.9103 - acc: 0.6030\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 1.2816 - acc: 0.1740\n",
      "\n",
      "4667/4667 [==============================] - 0s 6us/step - loss: 0.9072 - acc: 0.6008\n",
      "Epoch 4/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.2485 - acc: 0.2020Epoch 4/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 0.8833 - acc: 0.6260\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 0.8911 - acc: 0.6085\n",
      "\n",
      "4667/4667 [==============================] - 0s 7us/step - loss: 1.2300 - acc: 0.2078\n",
      "Epoch 5/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 0.8718 - acc: 0.6210Epoch 5/5\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.1999 - acc: 0.2170\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 1.1897 - acc: 0.2453\n",
      "\n",
      "4667/4667 [==============================] - 0s 6us/step - loss: 0.8633 - acc: 0.6381\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 0s\n",
      "2334/2334 [==============================] - 0s 180us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 0s 164us/step\n",
      "\n",
      "2333/2333 [==============================] - 0s 173us/step\n",
      "Epoch 1/2\n",
      "\n",
      "4666/4666 [==============================] - 2s 491us/step - loss: 1.2425 - acc: 0.1871\n",
      "Epoch 2/2\n",
      "\n",
      "4666/4666 [==============================] - 0s 2us/step - loss: 1.2397 - acc: 0.1886\n",
      "Epoch 1/2\n",
      "\n",
      "4667/4667 [==============================] - 2s 484us/step - loss: 1.4095 - acc: 0.3503\n",
      "Epoch 2/2\n",
      "\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 1.4001 - acc: 0.3493\n",
      "\n",
      "2334/2334 [==============================] - 0s 186us/step\n",
      "Epoch 1/2\n",
      "\n",
      "4667/4667 [==============================] - 2s 484us/step - loss: 1.0691 - acc: 0.4213\n",
      "Epoch 1/5\n",
      "\n",
      "4666/4666 [==============================] - 2s 487us/step - loss: 1.2202 - acc: 0.1106\n",
      "Epoch 2/5\n",
      "\n",
      "4666/4666 [==============================] - 0s 2us/step - loss: 1.2161 - acc: 0.1252\n",
      "Epoch 2/2\n",
      "\n",
      "4667/4667 [==============================] - 0s 3us/step - loss: 1.0659 - acc: 0.4187\n",
      "Epoch 3/5\n",
      "\n",
      "4666/4666 [==============================] - 0s 2us/step - loss: 1.2153 - acc: 0.1275\n",
      "Epoch 4/5\n",
      "\n",
      "4666/4666 [==============================] - 0s 2us/step - loss: 1.2056 - acc: 0.1414\n",
      "Epoch 5/5\n",
      "\n",
      "4666/4666 [==============================] - 0s 2us/step - loss: 1.1965 - acc: 0.1511\n",
      "\n",
      "2333/2333 [==============================] - 0s 192us/step\n",
      "\n",
      "2333/2333 [==============================] - 0s 193us/step\n",
      "\n",
      "2334/2334 [==============================] - 0s 194us/step\n",
      "Epoch 1/5\n",
      "\n",
      "4667/4667 [==============================] - 1s 251us/step - loss: 1.1179 - acc: 0.4285\n",
      "Epoch 2/5\n",
      "\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.1170 - acc: 0.4281\n",
      "Epoch 3/5\n",
      "\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.1095 - acc: 0.4393\n",
      "Epoch 4/5\n",
      "\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.1047 - acc: 0.4508\n",
      "Epoch 5/5\n",
      "\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.0977 - acc: 0.4515\n",
      "Epoch 1/5\n",
      "\n",
      "4667/4667 [==============================] - 1s 249us/step - loss: 1.3905 - acc: 0.4978\n",
      "Epoch 2/5\n",
      "\n",
      "4667/4667 [==============================] - 0s 2us/step - loss: 1.3895 - acc: 0.4995\n",
      "Epoch 3/5\n",
      "\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.3850 - acc: 0.4999\n",
      "Epoch 4/5\n",
      "\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.3671 - acc: 0.5016\n",
      "Epoch 5/5\n",
      "\n",
      "4667/4667 [==============================] - 0s 1us/step - loss: 1.3653 - acc: 0.4993\n",
      "\n",
      "2333/2333 [==============================] - 0s 107us/step\n",
      "\n",
      "2333/2333 [==============================] - 0s 92us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 13:48:51.951732 140020716218176 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 13:48:51.954348 140312447874880 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 13:48:51.967171 140020716218176 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 13:48:51.968374 140020716218176 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 13:48:51.974817 140020716218176 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 13:48:51.979039 140312447874880 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 13:48:51.980519 140312447874880 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 13:48:51.987223 140312447874880 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 13:48:51.990335 140020716218176 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 13:48:52.001104 140312447874880 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 13:48:52.028935 140020716218176 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 13:48:52.036275 140037298378560 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 13:48:52.039525 140312447874880 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 13:48:52.051186 140020716218176 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 13:48:52.051872 140037298378560 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 13:48:52.053031 140037298378560 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 13:48:52.059524 140037298378560 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 13:48:52.061903 140312447874880 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 13:48:52.073427 140037298378560 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 13:48:52.112112 140037298378560 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 13:48:52.133920 140037298378560 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 13:48:52.153513 140020716218176 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 13:48:52.163144 140312447874880 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 13:48:52.167516 140264853595968 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 13:48:52.188743 140264853595968 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 13:48:52.189916 140264853595968 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 13:48:52.196539 140264853595968 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 13:48:52.210601 140264853595968 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 13:48:52.236534 140037298378560 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 13:48:52.248938 140264853595968 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 13:48:52.270797 140264853595968 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 13:48:52.377532 140264853595968 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2019-07-14 13:48:52.389498: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 13:48:52.398267: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 13:48:52.403306: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 13:48:52.403510: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56253349d650 executing computations on platform Host. Devices:\n",
      "2019-07-14 13:48:52.403522: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "2019-07-14 13:48:52.413937: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 13:48:52.414097: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5594e4260e00 executing computations on platform Host. Devices:\n",
      "2019-07-14 13:48:52.414110: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 21395 tid 21395 thread 0 bound to OS proc set 0\n",
      "2019-07-14 13:48:52.414313: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 21394 tid 21394 thread 0 bound to OS proc set 0\n",
      "2019-07-14 13:48:52.425837: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 13:48:52.450095: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 13:48:52.454006: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 13:48:52.454130: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55717b5fd290 executing computations on platform Host. Devices:\n",
      "2019-07-14 13:48:52.454142: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 21396 tid 21396 thread 0 bound to OS proc set 0\n",
      "2019-07-14 13:48:52.454308: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 13:48:52.585798: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 13:48:52.589907: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 13:48:52.590043: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559894841a10 executing computations on platform Host. Devices:\n",
      "2019-07-14 13:48:52.590057: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 21400 tid 21400 thread 0 bound to OS proc set 0\n",
      "2019-07-14 13:48:52.590279: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 13:48:52.799056: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2019-07-14 13:48:52.815564: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2019-07-14 13:48:52.834677: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2019-07-14 13:48:52.910100: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21394 tid 21461 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21394 tid 21462 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21395 tid 21459 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21395 tid 21460 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21396 tid 21472 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21396 tid 21473 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21400 tid 21483 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21400 tid 21484 thread 2 bound to OS proc set 2\n",
      "2019-07-14 13:49:09.387016: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 13:49:09.410169: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 13:49:09.410539: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5591a0563c10 executing computations on platform Host. Devices:\n",
      "2019-07-14 13:49:09.410573: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21323 thread 0 bound to OS proc set 0\n",
      "2019-07-14 13:49:09.411261: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 13:49:09.534389: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21498 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21501 thread 3 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21500 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21502 thread 4 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21499 thread 5 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21504 thread 7 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21503 thread 6 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 21323 tid 21505 thread 8 bound to OS proc set 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "results = []\n",
    "grids = []\n",
    "for architecture in architectures : \n",
    "    layers = architecture\n",
    "    print(\"Using architecture: \", layers)\n",
    "    model = KerasClassifier(build_fn=build_DNN, batch_size=10000, epochs=2)\n",
    "    param_grid = dict(epochs=epochs, batch_size=batch_size)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                        n_jobs=-1, pre_dispatch=8)\n",
    "    grid_result = grid.fit(pcTrain, Y_Train)\n",
    "    results.append(grid_result)\n",
    "    grids.append(grid)\n",
    "    print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'epochs': 5}\n",
      "{'batch_size': 100, 'epochs': 5}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(architectures)) : \n",
    "    layers = architectures[i]\n",
    "    params = results[i].best_params_\n",
    "    print (params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
