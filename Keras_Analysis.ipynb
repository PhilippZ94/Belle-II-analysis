{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Belle2 MonteCarlo Data\n",
    "\n",
    "__Authors:__\n",
    "\n",
    "- Valeria Fioroni (University of Padova)\n",
    "- Matteo Guida (University of Padova)\n",
    "- Philipp Zehetner (University of Padova, Ludwig Maximilian University of Munich)\n",
    "- [Stack Overflow](https://stackoverflow.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pprint\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define branches that we need for our analysis\n",
    "Masses = ['B0_M', 'B0_ErrM', 'B0_SigM', 'B0_K_S0_M', 'B0_K_S0_ErrM', 'B0_K_S0_SigM', 'B0_etap_M', 'B0_etap_ErrM', \n",
    "          'B0_etap_SigM', 'B0_etap_eta_M', 'B0_etap_eta_ErrM', 'B0_etap_eta_SigM']\n",
    "Kinetics_CMS = ['B0_Pcms', 'B0_etap_Pcms', 'B0_etap_eta_Pcms', 'B0_etap_eta_gamma0_Pcms', 'B0_etap_eta_gamma1_Pcms',\n",
    "                'B0_etap_pi0_Pcms', 'B0_etap_pi1_Pcms', 'B0_K_S0_Pcms']\n",
    "Other_Kinetics = ['B0_deltae', 'B0_mbc']\n",
    "DecayAngles = ['B0_decayAngle__bo0__bc', 'B0_decayAngle__bo1__bc', 'B0_etap_decayAngle__bo0__bc',\n",
    "               'B0_etap_decayAngle__bo1__bc', 'B0_etap_decayAngle__bo2__bc']\n",
    "Positions = ['B0_X', 'B0_ErrX', 'B0_Y', 'B0_ErrY', 'B0_Z', 'B0_ErrZ', 'B0_Rho',  \n",
    "             'B0_etap_X', 'B0_etap_ErrX', 'B0_etap_Y', 'B0_etap_ErrY', \n",
    "             'B0_etap_Z', 'B0_etap_ErrZ', 'B0_etap_Rho',\n",
    "             'B0_etap_eta_X', 'B0_etap_eta_ErrX', 'B0_etap_eta_Y',\n",
    "             'B0_etap_eta_ErrY', 'B0_etap_eta_Z', 'B0_etap_eta_ErrZ', 'B0_etap_eta_Rho',\n",
    "             'B0_etap_pi0_X', 'B0_etap_pi0_ErrX', 'B0_etap_pi0_Y', 'B0_etap_pi0_ErrY', \n",
    "             'B0_etap_pi0_Z', 'B0_etap_pi0_ErrZ', 'B0_etap_pi0_Rho', \n",
    "             'B0_etap_pi1_X', 'B0_etap_pi1_ErrX', 'B0_etap_pi1_Y', 'B0_etap_pi1_ErrY', \n",
    "             'B0_etap_pi1_Z', 'B0_etap_pi1_ErrZ', 'B0_etap_pi1_Rho', \n",
    "             'B0_K_S0_X', 'B0_K_S0_ErrX', 'B0_K_S0_Y', 'B0_K_S0_ErrY', 'B0_K_S0_Z',\n",
    "             'B0_K_S0_ErrZ', 'B0_K_S0_Rho', \n",
    "             'B0_cosAngleBetweenMomentumAndVertexVector', 'B0_distance', 'B0_significanceOfDistance',\n",
    "             'B0_dr', 'B0_etap_pi0_dr', 'B0_etap_pi1_dr', 'B0_K_S0_dr']\n",
    "Vertex_Training = ['B0_VtxPvalue', 'B0_etap_VtxPvalue', 'B0_etap_eta_VtxPvalue', 'B0_etap_pi0_VtxPvalue',\n",
    "                   'B0_etap_pi1_VtxPvalue', 'B0_K_S0_VtxPvalue', ]\n",
    "Continuum_Suppression_Training = ['B0_TrCSMVA']\n",
    "\n",
    "Training = Kinetics_CMS + Masses + Other_Kinetics + Continuum_Suppression_Training + Positions + DecayAngles + Vertex_Training\n",
    "Important = Training + ['B0_isSignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.16/00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/root_numpy/__init__.py:46: RuntimeWarning: numpy 1.16.4 is currently installed but you installed root_numpy against numpy 1.9.3. Please consider reinstalling root_numpy for this numpy version.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from root_pandas import read_root\n",
    "\n",
    "path = '/home/philipp/Desktop/Project/DATA/'\n",
    "SFiles = glob.glob(os.path.join(path, 'Signal/*.root'))\n",
    "CFiles = glob.glob(os.path.join(path, 'Continuous/*.root'))\n",
    "PFiles = glob.glob(os.path.join(path, 'Peaking/*.root'))\n",
    "\n",
    "Full_Signal = pd.concat((read_root(f, 'B0', columns=Important) for f in SFiles))\n",
    "Full_Signal = Full_Signal[Full_Signal['B0_isSignal']==1].reset_index(drop=True)\n",
    "Full_Continuous = pd.concat((read_root(f, 'B0', columns=Important) for f in CFiles))\n",
    "Full_Peaking = pd.concat((read_root(f, 'B0', columns=Important) for f in PFiles))\n",
    "\n",
    "Signal = Full_Signal[Training]\n",
    "Continuous = Full_Continuous[Training]\n",
    "Peaking = Full_Peaking[Training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Signal['Type'] = 2\n",
    "Continuous['Type'] = 1\n",
    "Peaking['Type'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "n_seed=1234\n",
    "seed(n_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Sum_BS = pd.concat([Signal, Continuous, Peaking]).sample(frac=1)\n",
    "X = Sum_BS.drop('Type',axis=1)\n",
    "Y = Sum_BS['Type']\n",
    "\n",
    "# For Testing purposes it's convenient to work with a small subset\n",
    "if Testing : \n",
    "    N_Events = 10000\n",
    "    X = X[:N_Events]\n",
    "    Y = Y[:N_Events]\n",
    "\n",
    "# We split our dataset into 70% training, 15% testing and 15% validation \n",
    "X_Train, X_Rest, Y_Train, Y_Rest = train_test_split(X, Y, train_size=0.7,random_state=randint(10**6,10**9))\n",
    "X_Test, X_Validation, Y_Test, Y_Validation = train_test_split(X_Rest, Y_Rest, train_size=0.5,random_state=randint(10**6,10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_Train = ss.fit_transform(X_Train)\n",
    "X_Test = ss.transform(X_Test)\n",
    "X_Validation = ss.transform(X_Validation)\n",
    "\n",
    "Y_Train = to_categorical(Y_Train, num_classes=3) \n",
    "Y_Test = to_categorical(Y_Test, num_classes=3)\n",
    "Y_Validation = to_categorical(Y_Validation, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 variables explain 50% of the variance\n",
      "35 variables explain 90% of the variance\n",
      "41 variables explain 95% of the variance\n",
      "50 variables explain 99% of the variance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=len(Training))\n",
    "pcTrain = pd.DataFrame(data = pca.fit_transform(X_Train))\n",
    "pcTest = pd.DataFrame(data = pca.transform(X_Test))\n",
    "pcValidation = pd.DataFrame(data = pca.transform(X_Validation))\n",
    "\n",
    "T50, T90, T95, T99, = False, False, False, False\n",
    "for i in range(len(pca.explained_variance_ratio_)) : \n",
    "    if (sum(pca.explained_variance_ratio_[:i+1]) > 0.5 and T50 ==False) : \n",
    "        print(str(i+1) + ' variables explain 50% of the variance')\n",
    "        T50 = True\n",
    "        n_50 = i + 1 \n",
    "    if (sum(pca.explained_variance_ratio_[:i+1]) > 0.9 and T90 ==False) : \n",
    "        print(str(i+1) + ' variables explain 90% of the variance')\n",
    "        T90 = True\n",
    "        n_90 = i + 1\n",
    "    if (sum(pca.explained_variance_ratio_[:i+1]) > 0.95 and T95 ==False) : \n",
    "        print(str(i+1) + ' variables explain 95% of the variance')\n",
    "        T95 = True\n",
    "        n_95 = i + 1\n",
    "    if (sum(pca.explained_variance_ratio_[:i+1]) > 0.99 and T99 ==False) : \n",
    "        print(str(i+1) + ' variables explain 99% of the variance')\n",
    "        T99 = True\n",
    "        n_99 = i + 1\n",
    "        \n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For input_dim chose n_50, n_90, n_95 or n_99 depending on how much explained variance we require\n",
    "input_dim = n_99\n",
    "pcTrain = pcTrain.iloc[:, 0:input_dim]\n",
    "pcTest = pcTest.iloc[:, 0:input_dim]\n",
    "pcValidation = pcValidation.iloc[:, 0:input_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ['SGD', 'Adam']\n",
    "epochs = [10, 30]\n",
    "epochs = [1, 2]\n",
    "batch_size = [100, 1000]\n",
    "architectures = [ [50, 50], [50, 100, 50], [50, 100, 100, 50] ] \n",
    "activation = ['relu', 'elu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def build_DNN(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(pcTrain.shape[1],)))\n",
    "    model.add(Dense(layers[0], input_shape=(pcTrain.shape[1],), activation='relu'))\n",
    "    for i in range(1,len(layers)):\n",
    "        model.add(Dense(layers[i], activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def print_results() :\n",
    "    print(\"Layers: \", layers)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params): \n",
    "        print(\"%f (%f) with %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as the GridSearch does not provide the possibility to try out different architectures. \n",
    "As a workaround I suggest defining layers=[xx, xx, xx] for a couple of different architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using architecture:  [50, 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 19:51:26.817554 140236883281728 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 19:51:26.835836 140236883281728 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 19:51:26.838609 140236883281728 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 19:51:26.848325 140236883281728 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 19:51:26.860566 140236883281728 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 19:51:26.901732 140236883281728 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 19:51:26.921255 140236883281728 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 19:51:27.016816 140236883281728 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.7465 - acc: 0.6809\n",
      "Epoch 2/2\n",
      "7000/7000 [==============================] - 0s 20us/step - loss: 0.2830 - acc: 0.9029\n",
      "Layers:  [50, 50]\n",
      "Best: 0.958143 using {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.891857 (0.031474) with {'activation': 'relu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.914143 (0.002961) with {'activation': 'relu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.957000 (0.005075) with {'activation': 'relu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.955143 (0.005682) with {'activation': 'relu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.600143 (0.026727) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.506143 (0.137326) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.679000 (0.060339) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.573143 (0.102817) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.856286 (0.055710) with {'activation': 'elu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.879143 (0.028241) with {'activation': 'elu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.958143 (0.001571) with {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.953286 (0.006067) with {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.458429 (0.116923) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.467143 (0.137633) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.644857 (0.054690) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.640429 (0.076156) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "Using architecture:  [50, 100, 50]\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 1s 150us/step - loss: 0.7613 - acc: 0.6505\n",
      "2334/2334 [==============================] - 0s 45us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 1s 155us/step - loss: 0.7293 - acc: 0.6655\n",
      "2333/2333 [==============================] - 0s 49us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 1s 168us/step - loss: 0.7095 - acc: 0.6520\n",
      "2333/2333 [==============================] - 0s 55us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 1s 177us/step - loss: 0.7441 - acc: 0.6721\n",
      "2334/2334 [==============================] - 0s 72us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 1s 183us/step - loss: 0.6321 - acc: 0.7124\n",
      "2333/2333 [==============================] - 0s 70us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 1s 188us/step - loss: 0.7036 - acc: 0.7319\n",
      "2333/2333 [==============================] - 0s 82us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 1s 197us/step - loss: 0.6122 - acc: 0.7520\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 26us/step - loss: 0.2545 - acc: 0.9046\n",
      "2334/2334 [==============================] - 0s 87us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 204us/step - loss: 0.6475 - acc: 0.7484\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 26us/step - loss: 0.2735 - acc: 0.9059\n",
      "2333/2333 [==============================] - 0s 95us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 211us/step - loss: 0.6421 - acc: 0.7204\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 28us/step - loss: nan - acc: 0.4255\n",
      "2333/2333 [==============================] - 0s 101us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 1s 218us/step - loss: 0.5900 - acc: 0.7617\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 25us/step - loss: nan - acc: 0.7758\n",
      "2334/2334 [==============================] - 0s 105us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 228us/step - loss: 0.5911 - acc: 0.7720\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 26us/step - loss: 0.2585 - acc: 0.9121\n",
      "2333/2333 [==============================] - 0s 110us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 238us/step - loss: 0.7030 - acc: 0.7086\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 27us/step - loss: 0.2892 - acc: 0.8907\n",
      "2333/2333 [==============================] - 0s 120us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 1s 225us/step - loss: 1.0243 - acc: 0.5150\n",
      "2334/2334 [==============================] - 0s 120us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 1s 236us/step - loss: 1.0820 - acc: 0.4558\n",
      "2333/2333 [==============================] - 0s 125us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 1s 238us/step - loss: 0.9737 - acc: 0.5269\n",
      "2333/2333 [==============================] - 0s 135us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 1s 252us/step - loss: 0.9727 - acc: 0.5401\n",
      "2334/2334 [==============================] - 0s 142us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 1s 273us/step - loss: 1.0874 - acc: 0.4568\n",
      "2333/2333 [==============================] - 0s 156us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 1s 296us/step - loss: 0.9557 - acc: 0.5340\n",
      "2333/2333 [==============================] - 0s 175us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 1s 289us/step - loss: 0.9849 - acc: 0.5231\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 11us/step - loss: 0.8207 - acc: 0.5879\n",
      "2334/2334 [==============================] - 0s 190us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 328us/step - loss: 1.2233 - acc: 0.1039\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 10us/step - loss: 1.0348 - acc: 0.5089\n",
      "2333/2333 [==============================] - 0s 180us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 1s 319us/step - loss: 1.0826 - acc: 0.4800\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 8us/step - loss: 0.8838 - acc: 0.7283\n",
      "2333/2333 [==============================] - 0s 188us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 1s 317us/step - loss: 0.9575 - acc: 0.5658\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 7us/step - loss: 0.7713 - acc: 0.6985\n",
      "2334/2334 [==============================] - 1s 222us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 365us/step - loss: 0.9649 - acc: 0.5783\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 7us/step - loss: 0.7991 - acc: 0.7352\n",
      "2333/2333 [==============================] - 0s 209us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 364us/step - loss: 1.1103 - acc: 0.3362\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 10us/step - loss: 0.8941 - acc: 0.6010\n",
      "2333/2333 [==============================] - 1s 224us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 2s 412us/step - loss: 0.7016 - acc: 0.7077\n",
      "2334/2334 [==============================] - 1s 257us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 2s 386us/step - loss: 0.6324 - acc: 0.7454\n",
      "2333/2333 [==============================] - 1s 237us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 2s 397us/step - loss: 0.7309 - acc: 0.6366\n",
      "2333/2333 [==============================] - 1s 250us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 2s 397us/step - loss: 0.7037 - acc: 0.6912\n",
      "2334/2334 [==============================] - 1s 258us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 2s 438us/step - loss: 0.6103 - acc: 0.7086\n",
      "2333/2333 [==============================] - 1s 260us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 2s 427us/step - loss: 0.5758 - acc: 0.7022\n",
      "2333/2333 [==============================] - 1s 284us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 2s 429us/step - loss: 0.6177 - acc: 0.7660\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 32us/step - loss: 0.2653 - acc: 0.9068\n",
      "2334/2334 [==============================] - 1s 262us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 431us/step - loss: 0.6640 - acc: 0.6662\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4667/4667 [==============================] - 0s 36us/step - loss: 0.2809 - acc: 0.9004\n",
      "2333/2333 [==============================] - 1s 316us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 455us/step - loss: 0.6597 - acc: 0.7191\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 32us/step - loss: 0.2487 - acc: 0.9074\n",
      "2333/2333 [==============================] - 1s 298us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 2s 447us/step - loss: 0.6042 - acc: 0.7392\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 33us/step - loss: 0.2587 - acc: 0.9059\n",
      "2334/2334 [==============================] - 1s 302us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 504us/step - loss: 0.6210 - acc: 0.6662\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 32us/step - loss: nan - acc: 0.4633\n",
      "2333/2333 [==============================] - 1s 367us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 518us/step - loss: 0.7045 - acc: 0.6531\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 33us/step - loss: 0.2825 - acc: 0.8978\n",
      "2333/2333 [==============================] - 1s 337us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 2s 479us/step - loss: 1.0372 - acc: 0.4987\n",
      "2334/2334 [==============================] - 1s 327us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 2s 487us/step - loss: 0.9878 - acc: 0.5157\n",
      "2333/2333 [==============================] - 1s 330us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 2s 472us/step - loss: 1.1282 - acc: 0.2190\n",
      "2333/2333 [==============================] - 1s 351us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 3s 548us/step - loss: 1.0039 - acc: 0.4803\n",
      "2334/2334 [==============================] - 1s 347us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 2s 512us/step - loss: 0.9439 - acc: 0.4834\n",
      "2333/2333 [==============================] - 1s 346us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 2s 518us/step - loss: 1.0050 - acc: 0.5288\n",
      "2333/2333 [==============================] - 1s 354us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 2s 499us/step - loss: 1.0202 - acc: 0.4595\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 8us/step - loss: 0.8413 - acc: 0.5802\n",
      "2334/2334 [==============================] - 1s 359us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 505us/step - loss: 1.1802 - acc: 0.0321\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 10us/step - loss: 1.0511 - acc: 0.5108\n",
      "2333/2333 [==============================] - 1s 368us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 2s 518us/step - loss: 1.2165 - acc: 0.2816\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 8us/step - loss: 0.8702 - acc: 0.6336\n",
      "2333/2333 [==============================] - 1s 373us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 2s 526us/step - loss: 1.0311 - acc: 0.4934\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 9us/step - loss: 0.8350 - acc: 0.5853\n",
      "2334/2334 [==============================] - 1s 380us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 3s 570us/step - loss: 1.0167 - acc: 0.4583\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 7us/step - loss: 0.8243 - acc: 0.5787\n",
      "2333/2333 [==============================] - 1s 389us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 3s 548us/step - loss: 1.2472 - acc: 0.1583\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 8us/step - loss: 1.0298 - acc: 0.5580\n",
      "2333/2333 [==============================] - 1s 394us/step\n",
      "Epoch 1/2\n",
      "7000/7000 [==============================] - 3s 394us/step - loss: 0.6072 - acc: 0.7720\n",
      "Epoch 2/2\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.2231 - acc: 0.9284\n",
      "Layers:  [50, 100, 50]\n",
      "Best: 0.959714 using {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.925000 (0.002281) with {'activation': 'relu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.925143 (0.006797) with {'activation': 'relu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.641429 (0.451692) with {'activation': 'relu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.642429 (0.451638) with {'activation': 'relu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.603714 (0.030687) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.619857 (0.064371) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.722143 (0.096769) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.743143 (0.106493) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.931857 (0.009091) with {'activation': 'elu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.927000 (0.018202) with {'activation': 'elu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.959714 (0.004803) with {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.642571 (0.451906) with {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.573429 (0.041801) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.562286 (0.045738) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.727143 (0.073429) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.690857 (0.028325) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "Using architecture:  [50, 100, 100, 50]\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 3s 641us/step - loss: 0.6129 - acc: 0.7293\n",
      "2334/2334 [==============================] - 1s 431us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 3s 633us/step - loss: 0.6945 - acc: 0.7013\n",
      "2333/2333 [==============================] - 1s 441us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 3s 652us/step - loss: 0.5835 - acc: 0.6940\n",
      "2333/2333 [==============================] - 1s 453us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 3s 656us/step - loss: 0.5771 - acc: 0.7883\n",
      "2334/2334 [==============================] - 1s 464us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 3s 693us/step - loss: 0.5568 - acc: 0.7862\n",
      "2333/2333 [==============================] - 1s 494us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 3s 682us/step - loss: 0.5611 - acc: 0.8104\n",
      "2333/2333 [==============================] - 1s 483us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 3s 695us/step - loss: 0.6188 - acc: 0.6693\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 46us/step - loss: nan - acc: 0.8718\n",
      "2334/2334 [==============================] - 1s 493us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 3s 711us/step - loss: 0.5876 - acc: 0.7022\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 45us/step - loss: 0.2608 - acc: 0.9089\n",
      "2333/2333 [==============================] - 1s 505us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 3s 747us/step - loss: 0.6426 - acc: 0.6795\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 43us/step - loss: 0.2476 - acc: 0.9184\n",
      "2333/2333 [==============================] - 1s 536us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 4s 753us/step - loss: 0.6044 - acc: 0.7578\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 47us/step - loss: nan - acc: 0.6044\n",
      "2334/2334 [==============================] - 1s 542us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 4s 760us/step - loss: 0.5091 - acc: 0.7731\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 47us/step - loss: nan - acc: 0.0424\n",
      "2333/2333 [==============================] - 1s 536us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 4s 766us/step - loss: 0.6775 - acc: 0.6919\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 49us/step - loss: 0.2589 - acc: 0.9100\n",
      "2333/2333 [==============================] - 1s 550us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 3s 741us/step - loss: 0.9939 - acc: 0.5184\n",
      "2334/2334 [==============================] - 1s 559us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 4s 751us/step - loss: 0.9940 - acc: 0.5790\n",
      "2333/2333 [==============================] - 1s 559us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 4s 787us/step - loss: 1.0103 - acc: 0.5052\n",
      "2333/2333 [==============================] - 1s 579us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 4s 783us/step - loss: 1.1008 - acc: 0.4218\n",
      "2334/2334 [==============================] - 1s 596us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 4s 788us/step - loss: 1.0341 - acc: 0.5361\n",
      "2333/2333 [==============================] - 1s 583us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 4s 801us/step - loss: 1.0737 - acc: 0.4045\n",
      "2333/2333 [==============================] - 1s 604us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 4s 807us/step - loss: 1.0227 - acc: 0.5150\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 9us/step - loss: 0.8371 - acc: 0.6395\n",
      "2334/2334 [==============================] - 1s 610us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 4s 826us/step - loss: 1.0550 - acc: 0.4553\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 9us/step - loss: 0.8785 - acc: 0.5948\n",
      "2333/2333 [==============================] - 1s 629us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 4s 840us/step - loss: 1.0918 - acc: 0.3739\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 10us/step - loss: 0.8938 - acc: 0.5410\n",
      "2333/2333 [==============================] - 2s 650us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 4s 846us/step - loss: 1.0923 - acc: 0.3506\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 10us/step - loss: 0.9023 - acc: 0.6813\n",
      "2334/2334 [==============================] - 2s 666us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 4s 856us/step - loss: 0.9309 - acc: 0.5307\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 9us/step - loss: 0.7446 - acc: 0.5657\n",
      "2333/2333 [==============================] - 2s 664us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 4s 873us/step - loss: 1.0632 - acc: 0.4260\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 9us/step - loss: 0.8415 - acc: 0.5704\n",
      "2333/2333 [==============================] - 2s 675us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 4s 924us/step - loss: 0.5996 - acc: 0.7964\n",
      "2334/2334 [==============================] - 2s 704us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 4s 939us/step - loss: nan - acc: 0.6960\n",
      "2333/2333 [==============================] - 2s 722us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 4s 954us/step - loss: 0.5847 - acc: 0.7534\n",
      "2333/2333 [==============================] - 2s 738us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 4s 961us/step - loss: 0.6614 - acc: 0.6942\n",
      "2334/2334 [==============================] - 2s 741us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 5s 991us/step - loss: 0.6612 - acc: 0.7570\n",
      "2333/2333 [==============================] - 2s 761us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 0.5692 - acc: 0.7806\n",
      "2333/2333 [==============================] - 2s 765us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 7s 1ms/step - loss: 0.5496 - acc: 0.7795\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 57us/step - loss: 0.2500 - acc: 0.9076\n",
      "2334/2334 [==============================] - 2s 792us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 0.5978 - acc: 0.7632\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 57us/step - loss: 0.2574 - acc: 0.9096\n",
      "2333/2333 [==============================] - 2s 789us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 0.6451 - acc: 0.7133\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 54us/step - loss: 0.2677 - acc: 0.9038\n",
      "2333/2333 [==============================] - 2s 800us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 5s 1ms/step - loss: 0.6412 - acc: 0.6449\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 57us/step - loss: 0.2553 - acc: 0.9115\n",
      "2334/2334 [==============================] - 2s 818us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 0.5823 - acc: 0.7390\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 57us/step - loss: nan - acc: 0.7165\n",
      "2333/2333 [==============================] - 2s 827us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 0.5303 - acc: 0.7838\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 56us/step - loss: 0.2485 - acc: 0.9040\n",
      "2333/2333 [==============================] - 2s 833us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 5s 1ms/step - loss: 0.9523 - acc: 0.5084\n",
      "2334/2334 [==============================] - 2s 809us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 1.0428 - acc: 0.5337\n",
      "2333/2333 [==============================] - 2s 814us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 1.0233 - acc: 0.5811\n",
      "2333/2333 [==============================] - 2s 835us/step\n",
      "Epoch 1/1\n",
      "4666/4666 [==============================] - 5s 1ms/step - loss: 1.0625 - acc: 0.3770\n",
      "2334/2334 [==============================] - 2s 841us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 1.0222 - acc: 0.4973\n",
      "2333/2333 [==============================] - 2s 845us/step\n",
      "Epoch 1/1\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 0.9868 - acc: 0.5014\n",
      "2333/2333 [==============================] - 2s 856us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 5s 1ms/step - loss: 0.9927 - acc: 0.5997\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 11us/step - loss: 0.8152 - acc: 0.7173\n",
      "2334/2334 [==============================] - 2s 876us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 1.0327 - acc: 0.5717\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 9us/step - loss: 0.8160 - acc: 0.7889\n",
      "2333/2333 [==============================] - 2s 885us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 1.0265 - acc: 0.4840\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 10us/step - loss: 0.8411 - acc: 0.5160\n",
      "2333/2333 [==============================] - 2s 884us/step\n",
      "Epoch 1/2\n",
      "4666/4666 [==============================] - 5s 1ms/step - loss: 1.0369 - acc: 0.5086\n",
      "Epoch 2/2\n",
      "4666/4666 [==============================] - 0s 10us/step - loss: 0.8414 - acc: 0.7319\n",
      "2334/2334 [==============================] - 2s 900us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 6s 1ms/step - loss: 0.9902 - acc: 0.5385\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 12us/step - loss: 0.8010 - acc: 0.7373\n",
      "2333/2333 [==============================] - 2s 924us/step\n",
      "Epoch 1/2\n",
      "4667/4667 [==============================] - 5s 1ms/step - loss: 1.0134 - acc: 0.4866\n",
      "Epoch 2/2\n",
      "4667/4667 [==============================] - 0s 12us/step - loss: 0.7805 - acc: 0.6630\n",
      "2333/2333 [==============================] - 2s 946us/step\n",
      "Epoch 1/2\n",
      "7000/7000 [==============================] - 6s 844us/step - loss: 0.4570 - acc: 0.8120\n",
      "Epoch 2/2\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.2259 - acc: 0.9193\n",
      "Layers:  [50, 100, 100, 50]\n",
      "Best: 0.962429 using {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.933000 (0.009808) with {'activation': 'relu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.937714 (0.008021) with {'activation': 'relu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.642143 (0.451493) with {'activation': 'relu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.325000 (0.454517) with {'activation': 'relu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.618143 (0.074931) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.668286 (0.072410) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.653429 (0.090054) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.659286 (0.056229) with {'activation': 'relu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.630857 (0.443701) with {'activation': 'elu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.925000 (0.017375) with {'activation': 'elu', 'batch_size': 100, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.962429 (0.003734) with {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.643286 (0.452400) with {'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "0.625857 (0.063411) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'SGD'}\n",
      "0.577571 (0.046241) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 1, 'optimizer': 'Adam'}\n",
      "0.707000 (0.148443) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.807143 (0.033373) with {'activation': 'elu', 'batch_size': 1000, 'epochs': 2, 'optimizer': 'Adam'}\n",
      "Epoch 1/1\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:31 - loss: 0.9830 - acc: 0.5400Epoch 1/1\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:31 - loss: 1.0346 - acc: 0.4600Epoch 1/1\n",
      "\n",
      " 100/4666 [..............................] - ETA: 1:33 - loss: 1.1603 - acc: 0.2200\n",
      "2900/4667 [=================>............] - ETA: 1s - loss: 0.8115 - acc: 0.6203  \n",
      "1700/4667 [=========>....................] - ETA: 3s - loss: 0.9132 - acc: 0.6100  \n",
      "2200/4666 [=============>................] - ETA: 2s - loss: 0.9717 - acc: 0.5109  \n",
      "4667/4667 [==============================] - 2s 449us/step - loss: 0.7210 - acc: 0.6848\n",
      "Epoch 1/1\n",
      "\n",
      " 100/4666 [..............................] - ETA: 1:32 - loss: 1.2190 - acc: 0.2700\n",
      "3900/4667 [========================>.....] - ETA: 0s - loss: 0.7482 - acc: 0.7326\n",
      "4400/4666 [===========================>..] - ETA: 0s - loss: 0.8398 - acc: 0.6057\n",
      "4666/4666 [==============================] - 2s 463us/step - loss: 0.8265 - acc: 0.6159\n",
      "\n",
      "2000/4666 [===========>..................] - ETA: 2s - loss: 0.9815 - acc: 0.5095  \n",
      "4667/4667 [==============================] - 2s 457us/step - loss: 0.7058 - acc: 0.7495\n",
      "\n",
      "4500/4666 [===========================>..] - ETA: 0s - loss: 0.7735 - acc: 0.6673\n",
      "4666/4666 [==============================] - 2s 459us/step - loss: 0.7624 - acc: 0.6745\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 6s\n",
      "2333/2333 [==============================] - 0s 145us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 6s\n",
      " 100/2334 [>.............................] - ETA: 7s\n",
      " 100/2334 [>.............................] - ETA: 6s\n",
      "2333/2333 [==============================] - 0s 141us/step\n",
      "\n",
      "2334/2334 [==============================] - 0s 154us/step\n",
      "\n",
      "2334/2334 [==============================] - 0s 138us/step\n",
      "Epoch 1/1\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:36 - loss: 1.0625 - acc: 0.5100\n",
      "2500/4667 [===============>..............] - ETA: 1s - loss: 0.7919 - acc: 0.7008  Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:35 - loss: 1.2305 - acc: 0.2800Epoch 1/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 1:35 - loss: 1.0321 - acc: 0.4800Epoch 1/1\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:36 - loss: 1.2226 - acc: 0.3100\n",
      "4667/4667 [==============================] - 2s 471us/step - loss: 0.6687 - acc: 0.7559\n",
      "\n",
      "2000/4667 [===========>..................] - ETA: 2s - loss: 0.9821 - acc: 0.5135  \n",
      "1900/4666 [===========>..................] - ETA: 3s - loss: 0.8540 - acc: 0.6553  \n",
      "2000/4667 [===========>..................] - ETA: 2s - loss: 0.9727 - acc: 0.5685  \n",
      "4000/4667 [========================>.....] - ETA: 0s - loss: 0.8542 - acc: 0.6285\n",
      "3800/4666 [=======================>......] - ETA: 0s - loss: 0.7435 - acc: 0.7313\n",
      "3800/4667 [=======================>......] - ETA: 0s - loss: 0.8234 - acc: 0.6795\n",
      "4667/4667 [==============================] - 2s 473us/step - loss: 0.8212 - acc: 0.6555\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.5969 - acc: 0.7400\n",
      "4666/4666 [==============================] - 2s 474us/step - loss: 0.6986 - acc: 0.7527\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 0.4003 - acc: 0.8800\n",
      "4667/4667 [==============================] - 2s 481us/step - loss: 0.7698 - acc: 0.7075\n",
      "\n",
      "2300/4667 [=============>................] - ETA: 0s - loss: 0.4684 - acc: 0.8639\n",
      "2100/4666 [============>.................] - ETA: 0s - loss: 0.3566 - acc: 0.8771\n",
      "4400/4667 [===========================>..] - ETA: 0s - loss: 0.3910 - acc: 0.8827\n",
      "4000/4666 [========================>.....] - ETA: 0s - loss: 0.3214 - acc: 0.8893\n",
      "4667/4667 [==============================] - 0s 27us/step - loss: 0.3807 - acc: 0.8858\n",
      "\n",
      "4666/4666 [==============================] - 0s 26us/step - loss: 0.3144 - acc: 0.8920\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 8s\n",
      "2333/2333 [==============================] - 0s 168us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 8s\n",
      "2333/2333 [==============================] - 0s 170us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 8s\n",
      " 100/2334 [>.............................] - ETA: 8s\n",
      "2333/2333 [==============================] - 0s 171us/step\n",
      "\n",
      "2334/2334 [==============================] - 0s 175us/step\n",
      "Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:38 - loss: 1.3809 - acc: 0.2200\n",
      "2500/4667 [===============>..............] - ETA: 1s - loss: 0.9932 - acc: 0.5496  \n",
      "4667/4667 [==============================] - 2s 484us/step - loss: 0.8384 - acc: 0.6750\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.5832 - acc: 0.8300\n",
      "2600/4667 [===============>..............] - ETA: 0s - loss: 0.4502 - acc: 0.8769Epoch 1/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 1:42 - loss: 1.3104 - acc: 0.4700\n",
      "4667/4667 [==============================] - 0s 22us/step - loss: 0.3918 - acc: 0.8875\n",
      "\n",
      "1800/4666 [==========>...................] - ETA: 3s - loss: 1.0631 - acc: 0.5167  \n",
      "4600/4666 [============================>.] - ETA: 0s - loss: 0.8516 - acc: 0.6428\n",
      "4666/4666 [==============================] - 2s 503us/step - loss: 0.8479 - acc: 0.6462\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 0.5563 - acc: 0.8200Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:43 - loss: 0.9817 - acc: 0.4200Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 1:43 - loss: 1.1671 - acc: 0.2700\n",
      "2600/4666 [===============>..............] - ETA: 0s - loss: 0.4708 - acc: 0.8512\n",
      "4666/4666 [==============================] - 0s 22us/step - loss: 0.4055 - acc: 0.8654\n",
      "\n",
      "1900/4667 [===========>..................] - ETA: 3s - loss: 0.9643 - acc: 0.5311  \n",
      "2000/4667 [===========>..................] - ETA: 3s - loss: 0.8557 - acc: 0.5505  \n",
      "3900/4667 [========================>.....] - ETA: 0s - loss: 0.8081 - acc: 0.6267\n",
      "4000/4667 [========================>.....] - ETA: 0s - loss: 0.7790 - acc: 0.6217\n",
      "4667/4667 [==============================] - 2s 513us/step - loss: 0.7631 - acc: 0.6550\n",
      "\n",
      "4667/4667 [==============================] - 2s 515us/step - loss: 0.7531 - acc: 0.6480\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.4995 - acc: 0.8200Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.5478 - acc: 0.8500\n",
      "2200/4667 [=============>................] - ETA: 0s - loss: 0.4200 - acc: 0.8705\n",
      "2200/4667 [=============>................] - ETA: 0s - loss: 0.4610 - acc: 0.8564\n",
      "4100/4667 [=========================>....] - ETA: 0s - loss: 0.3632 - acc: 0.8861\n",
      "4100/4667 [=========================>....] - ETA: 0s - loss: 0.4011 - acc: 0.8698\n",
      "4667/4667 [==============================] - 0s 25us/step - loss: 0.3566 - acc: 0.8873\n",
      "\n",
      "4667/4667 [==============================] - 0s 26us/step - loss: 0.3842 - acc: 0.8751\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 9s\n",
      "2333/2333 [==============================] - 0s 190us/step\n",
      "\n",
      " 100/2334 [>.............................] - ETA: 9s\n",
      "2334/2334 [==============================] - 0s 194us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 9s\n",
      " 100/2333 [>.............................] - ETA: 9s\n",
      "2333/2333 [==============================] - 0s 188us/step\n",
      "\n",
      "2333/2333 [==============================] - 0s 193us/step\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 8s - loss: 0.9688 - acc: 0.4800\n",
      "4666/4666 [==============================] - 2s 501us/step - loss: 0.9302 - acc: 0.5054\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 8s - loss: 1.0372 - acc: 0.5500\n",
      "4667/4667 [==============================] - 2s 504us/step - loss: 1.0120 - acc: 0.5395\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 8s - loss: 1.3623 - acc: 0.1000\n",
      "4666/4666 [==============================] - 2s 505us/step - loss: 1.2786 - acc: 0.1695\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 8s - loss: 0.9315 - acc: 0.5550\n",
      "4667/4667 [==============================] - 2s 511us/step - loss: 0.9010 - acc: 0.5869\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 0s\n",
      "2334/2334 [==============================] - 0s 195us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 0s 208us/step\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 0s\n",
      "2334/2334 [==============================] - 0s 203us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 0s 202us/step\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 8s - loss: 1.0320 - acc: 0.5020\n",
      "4667/4667 [==============================] - 2s 522us/step - loss: 0.9747 - acc: 0.5635\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 8s - loss: 0.9945 - acc: 0.4860\n",
      "4667/4667 [==============================] - 2s 527us/step - loss: 0.9393 - acc: 0.5022\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 8s - loss: 1.0393 - acc: 0.4970\n",
      "4666/4666 [==============================] - 2s 527us/step - loss: 0.9853 - acc: 0.5581\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 0.9171 - acc: 0.6310\n",
      "4666/4666 [==============================] - 0s 5us/step - loss: 0.8637 - acc: 0.6599\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 8s - loss: 1.3014 - acc: 0.1090\n",
      "4667/4667 [==============================] - 2s 524us/step - loss: 1.2337 - acc: 0.1916\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.1301 - acc: 0.3380\n",
      "4667/4667 [==============================] - 0s 5us/step - loss: 1.0593 - acc: 0.4375\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 1s 222us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 1s 227us/step\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 0s\n",
      "2334/2334 [==============================] - 1s 223us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 1s 222us/step\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 9s - loss: 0.9734 - acc: 0.5130\n",
      "4667/4667 [==============================] - 3s 585us/step - loss: 0.9320 - acc: 0.5427\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 0.8603 - acc: 0.5920\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 0.8273 - acc: 0.6366\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 9s - loss: 1.1037 - acc: 0.3820\n",
      "4666/4666 [==============================] - 3s 584us/step - loss: 1.0603 - acc: 0.4340\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 0.9601 - acc: 0.5510\n",
      "4666/4666 [==============================] - 0s 4us/step - loss: 0.9317 - acc: 0.5784\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 9s - loss: 1.2465 - acc: 0.2090\n",
      "4667/4667 [==============================] - 3s 582us/step - loss: 1.1968 - acc: 0.2488\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.0859 - acc: 0.3610\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 1.0504 - acc: 0.3692\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 10s - loss: 1.2221 - acc: 0.4010\n",
      "4667/4667 [==============================] - 3s 588us/step - loss: 1.1549 - acc: 0.4234\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.0064 - acc: 0.5230\n",
      "4667/4667 [==============================] - 0s 6us/step - loss: 0.9633 - acc: 0.5310\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 1s 250us/step\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 0s\n",
      "2334/2334 [==============================] - 1s 245us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 1s 253us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 1s 253us/step\n",
      "Epoch 1/1\n",
      "\n",
      " 100/4666 [..............................] - ETA: 2:02 - loss: 1.1390 - acc: 0.3600\n",
      "2100/4666 [============>.................] - ETA: 3s - loss: 0.8649 - acc: 0.5914  \n",
      "4300/4666 [==========================>...] - ETA: 0s - loss: 0.7319 - acc: 0.6967Epoch 1/1\n",
      "\n",
      " 100/4667 [..............................] - ETA: 2:03 - loss: 1.4087 - acc: 0.1300\n",
      "4666/4666 [==============================] - 3s 597us/step - loss: 0.7158 - acc: 0.7090\n",
      "\n",
      "1700/4667 [=========>....................] - ETA: 4s - loss: 1.1357 - acc: 0.3182  \n",
      "4000/4667 [========================>.....] - ETA: 0s - loss: 0.9369 - acc: 0.4972\n",
      "4667/4667 [==============================] - 3s 603us/step - loss: 0.8983 - acc: 0.5243\n",
      "Epoch 1/1\n",
      "\n",
      " 100/4667 [..............................] - ETA: 2:04 - loss: 1.2092 - acc: 0.3900\n",
      "2300/4667 [=============>................] - ETA: 2s - loss: 0.9325 - acc: 0.5270  Epoch 1/1\n",
      "\n",
      " 100/4666 [..............................] - ETA: 2:05 - loss: 1.0811 - acc: 0.3800\n",
      "4600/4667 [============================>.] - ETA: 0s - loss: 0.7889 - acc: 0.6498\n",
      "4667/4667 [==============================] - 3s 605us/step - loss: 0.7861 - acc: 0.6527\n",
      "\n",
      "1900/4666 [===========>..................] - ETA: 4s - loss: 0.8937 - acc: 0.5916  \n",
      "4100/4666 [=========================>....] - ETA: 0s - loss: 0.7635 - acc: 0.6954\n",
      "4666/4666 [==============================] - 3s 612us/step - loss: 0.7344 - acc: 0.7143\n",
      "\n",
      " 100/2334 [>.............................] - ETA: 14s\n",
      "2334/2334 [==============================] - 1s 291us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 13s\n",
      "2333/2333 [==============================] - 1s 284us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 14s\n",
      "2333/2333 [==============================] - 1s 296us/step\n",
      "\n",
      " 100/2334 [>.............................] - ETA: 14s\n",
      "2334/2334 [==============================] - 1s 291us/step\n",
      "Epoch 1/1\n",
      "\n",
      " 100/4667 [..............................] - ETA: 2:11 - loss: 1.0777 - acc: 0.3400\n",
      "2200/4667 [=============>................] - ETA: 3s - loss: 0.8501 - acc: 0.5373  \n",
      "4400/4667 [===========================>..] - ETA: 0s - loss: 0.7291 - acc: 0.6495\n",
      "4667/4667 [==============================] - 3s 638us/step - loss: 0.7178 - acc: 0.6593\n",
      "Epoch 1/1\n",
      "\n",
      " 100/4667 [..............................] - ETA: 2:12 - loss: 1.3000 - acc: 0.0300\n",
      "2400/4667 [==============>...............] - ETA: 2s - loss: 1.0604 - acc: 0.4429  \n",
      "4667/4667 [==============================] - 3s 646us/step - loss: 0.8934 - acc: 0.5959\n",
      "Epoch 1/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 2:12 - loss: 0.9716 - acc: 0.5700Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 2:09 - loss: 1.0284 - acc: 0.4500\n",
      "2300/4666 [=============>................] - ETA: 3s - loss: 0.8026 - acc: 0.6787  \n",
      "1800/4667 [==========>...................] - ETA: 4s - loss: 0.8330 - acc: 0.6550  \n",
      "4100/4666 [=========================>....] - ETA: 0s - loss: 0.7021 - acc: 0.7437\n",
      "4666/4666 [==============================] - 3s 646us/step - loss: 0.6751 - acc: 0.7568\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 0.3921 - acc: 0.9200\n",
      "3600/4667 [======================>.......] - ETA: 0s - loss: 0.7047 - acc: 0.7311\n",
      "4667/4667 [==============================] - 3s 640us/step - loss: 0.6426 - acc: 0.7602\n",
      "\n",
      "2000/4666 [===========>..................] - ETA: 0s - loss: 0.3430 - acc: 0.8900Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.4336 - acc: 0.8500\n",
      "3900/4666 [========================>.....] - ETA: 0s - loss: 0.3158 - acc: 0.8972\n",
      "2000/4667 [===========>..................] - ETA: 0s - loss: 0.3302 - acc: 0.8950\n",
      "4666/4666 [==============================] - 0s 30us/step - loss: 0.3049 - acc: 0.9012\n",
      "\n",
      "3500/4667 [=====================>........] - ETA: 0s - loss: 0.3102 - acc: 0.9006\n",
      "4667/4667 [==============================] - 0s 27us/step - loss: 0.2974 - acc: 0.9038\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 15s\n",
      "2333/2333 [==============================] - 1s 310us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 14s\n",
      "2333/2333 [==============================] - 1s 294us/step\n",
      "\n",
      " 100/2334 [>.............................] - ETA: 15s\n",
      " 100/2333 [>.............................] - ETA: 15s\n",
      "2334/2334 [==============================] - 1s 316us/step\n",
      "\n",
      "2200/2333 [===========================>..] - ETA: 0s \n",
      "2333/2333 [==============================] - 1s 311us/step\n",
      "Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 2:11 - loss: 1.1288 - acc: 0.4300\n",
      "1700/4667 [=========>....................] - ETA: 5s - loss: 0.9856 - acc: 0.5753  \n",
      "3800/4667 [=======================>......] - ETA: 0s - loss: 0.8301 - acc: 0.7003\n",
      "4667/4667 [==============================] - 3s 645us/step - loss: 0.7754 - acc: 0.7317\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.4938 - acc: 0.8400Epoch 1/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 2:15 - loss: 1.0106 - acc: 0.4000\n",
      "2500/4667 [===============>..............] - ETA: 0s - loss: 0.3746 - acc: 0.8808\n",
      "1400/4666 [========>.....................] - ETA: 7s - loss: 0.9094 - acc: 0.4457  \n",
      "4400/4667 [===========================>..] - ETA: 0s - loss: 0.3351 - acc: 0.8893\n",
      "4667/4667 [==============================] - 0s 27us/step - loss: 0.3330 - acc: 0.8890\n",
      "\n",
      "3200/4666 [===================>..........] - ETA: 1s - loss: 0.7877 - acc: 0.5731\n",
      "4666/4666 [==============================] - 3s 663us/step - loss: 0.7134 - acc: 0.6509\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4666 [..............................] - ETA: 0s - loss: 0.4481 - acc: 0.8900\n",
      "2000/4666 [===========>..................] - ETA: 0s - loss: 0.4011 - acc: 0.8630\n",
      "4600/4666 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8837\n",
      "4666/4666 [==============================] - 0s 24us/step - loss: 0.3399 - acc: 0.8841\n",
      "Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 2:16 - loss: 1.2966 - acc: 0.1800\n",
      "1800/4667 [==========>...................] - ETA: 4s - loss: 1.0362 - acc: 0.4456  \n",
      "3900/4667 [========================>.....] - ETA: 0s - loss: 0.8570 - acc: 0.6044Epoch 1/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 2:18 - loss: 1.5270 - acc: 0.3900\n",
      "4667/4667 [==============================] - 3s 664us/step - loss: 0.8099 - acc: 0.6400\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.5332 - acc: 0.8700\n",
      "1700/4667 [=========>....................] - ETA: 5s - loss: 1.1323 - acc: 0.5006  \n",
      "1600/4667 [=========>....................] - ETA: 0s - loss: 0.4736 - acc: 0.8712\n",
      "3600/4667 [======================>.......] - ETA: 0s - loss: 0.9088 - acc: 0.6311\n",
      "3500/4667 [=====================>........] - ETA: 0s - loss: 0.4258 - acc: 0.8671\n",
      "4667/4667 [==============================] - 3s 680us/step - loss: 0.8310 - acc: 0.6698\n",
      "Epoch 2/2\n",
      "\n",
      " 100/4667 [..............................] - ETA: 0s - loss: 0.5724 - acc: 0.8300\n",
      "4667/4667 [==============================] - 0s 31us/step - loss: 0.3950 - acc: 0.8734\n",
      "\n",
      "2000/4667 [===========>..................] - ETA: 0s - loss: 0.4539 - acc: 0.8585\n",
      "4300/4667 [==========================>...] - ETA: 0s - loss: 0.3873 - acc: 0.8767\n",
      "4667/4667 [==============================] - 0s 25us/step - loss: 0.3789 - acc: 0.8796\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 17s\n",
      "2333/2333 [==============================] - 1s 349us/step\n",
      "\n",
      " 100/2334 [>.............................] - ETA: 16s\n",
      "2334/2334 [==============================] - 1s 330us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 16s\n",
      "2333/2333 [==============================] - 1s 337us/step\n",
      "\n",
      " 100/2333 [>.............................] - ETA: 16s\n",
      "2333/2333 [==============================] - 1s 331us/step\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 11s - loss: 1.2200 - acc: 0.1410\n",
      "4666/4666 [==============================] - 3s 656us/step - loss: 1.1741 - acc: 0.1989\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 11s - loss: 1.0218 - acc: 0.5080\n",
      "4667/4667 [==============================] - 3s 662us/step - loss: 0.9692 - acc: 0.5535\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 11s - loss: 1.2828 - acc: 0.3000\n",
      "4667/4667 [==============================] - 3s 664us/step - loss: 1.2091 - acc: 0.3488\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 11s - loss: 1.2169 - acc: 0.2750\n",
      "4666/4666 [==============================] - 3s 677us/step - loss: 1.1448 - acc: 0.3495\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 1s\n",
      "2334/2334 [==============================] - 1s 335us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 1s\n",
      "2333/2333 [==============================] - 1s 341us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 1s\n",
      "2333/2333 [==============================] - 1s 341us/step\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 1s\n",
      "2334/2334 [==============================] - 1s 346us/step\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 12s - loss: 1.0389 - acc: 0.4600\n",
      "4667/4667 [==============================] - 3s 710us/step - loss: 0.9935 - acc: 0.5121\n",
      "Epoch 1/1\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 11s - loss: 1.2127 - acc: 0.1320\n",
      "4667/4667 [==============================] - 3s 706us/step - loss: 1.1687 - acc: 0.1853\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 12s - loss: 0.9931 - acc: 0.4990\n",
      "4666/4666 [==============================] - 3s 717us/step - loss: 0.9565 - acc: 0.5150\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 0.8986 - acc: 0.5690\n",
      "4666/4666 [==============================] - 0s 6us/step - loss: 0.8713 - acc: 0.5744\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 12s - loss: 1.2511 - acc: 0.1920\n",
      "4667/4667 [==============================] - 3s 717us/step - loss: 1.1646 - acc: 0.2831\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.0525 - acc: 0.4300\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 1.0005 - acc: 0.4954\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 1s\n",
      "2333/2333 [==============================] - 1s 361us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 1s\n",
      "2333/2333 [==============================] - 1s 360us/step\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 1s\n",
      "2334/2334 [==============================] - 1s 370us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 1s\n",
      "2333/2333 [==============================] - 1s 373us/step\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 12s - loss: 0.9174 - acc: 0.5490\n",
      "4667/4667 [==============================] - 3s 724us/step - loss: 0.8890 - acc: 0.5582\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 0.8252 - acc: 0.5980\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 0.7942 - acc: 0.6430\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 12s - loss: 0.9985 - acc: 0.5430\n",
      "4666/4666 [==============================] - 3s 717us/step - loss: 0.9621 - acc: 0.5819\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4666 [=====>........................] - ETA: 0s - loss: 0.9031 - acc: 0.6290\n",
      "4666/4666 [==============================] - 0s 6us/step - loss: 0.8659 - acc: 0.6717\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 12s - loss: 1.0930 - acc: 0.3280\n",
      "4667/4667 [==============================] - 3s 721us/step - loss: 1.0405 - acc: 0.3805\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 0.9684 - acc: 0.4520\n",
      "4667/4667 [==============================] - 0s 4us/step - loss: 0.9160 - acc: 0.5132\n",
      "Epoch 1/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 12s - loss: 1.1373 - acc: 0.3110\n",
      "4667/4667 [==============================] - 3s 735us/step - loss: 1.0891 - acc: 0.3658\n",
      "Epoch 2/2\n",
      "\n",
      "1000/4667 [=====>........................] - ETA: 0s - loss: 1.0078 - acc: 0.4730\n",
      "4667/4667 [==============================] - 0s 6us/step - loss: 0.9712 - acc: 0.5172\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 1s\n",
      "2333/2333 [==============================] - 1s 394us/step\n",
      "\n",
      "1000/2334 [===========>..................] - ETA: 1s\n",
      "2334/2334 [==============================] - 1s 384us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 1s\n",
      "2333/2333 [==============================] - 1s 328us/step\n",
      "\n",
      "1000/2333 [===========>..................] - ETA: 0s\n",
      "2333/2333 [==============================] - 1s 287us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 19:50:31.310363 140586342672192 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 19:50:31.324429 140586342672192 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 19:50:31.325693 140586342672192 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 19:50:31.333215 140586342672192 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 19:50:31.336391 140043372750656 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 19:50:31.347913 140586342672192 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 19:50:31.351662 140043372750656 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 19:50:31.352837 140043372750656 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 19:50:31.355613 140343320893248 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 19:50:31.359349 140043372750656 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 19:50:31.370889 140343320893248 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 19:50:31.372018 140343320893248 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 19:50:31.372390 140043372750656 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 19:50:31.378461 140343320893248 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 19:50:31.387839 140586342672192 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 19:50:31.392355 140343320893248 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 19:50:31.409093 140409943414592 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0714 19:50:31.410668 140043372750656 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 19:50:31.412275 140586342672192 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 19:50:31.425395 140409943414592 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0714 19:50:31.426606 140409943414592 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0714 19:50:31.430946 140343320893248 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 19:50:31.432233 140043372750656 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 19:50:31.433313 140409943414592 deprecation.py:506] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0714 19:50:31.448148 140409943414592 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0714 19:50:31.452807 140343320893248 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 19:50:31.489475 140409943414592 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0714 19:50:31.518004 140409943414592 deprecation_wrapper.py:119] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0714 19:50:31.522802 140586342672192 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 19:50:31.533734 140043372750656 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 19:50:31.562228 140343320893248 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0714 19:50:31.622749 140409943414592 deprecation.py:323] From /home/philipp/anaconda3/envs/Belle/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2019-07-14 19:50:31.805610: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 19:50:31.814000: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 19:50:31.820625: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 19:50:31.820784: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c43fcc46a0 executing computations on platform Host. Devices:\n",
      "2019-07-14 19:50:31.820799: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 27515 tid 27515 thread 0 bound to OS proc set 0\n",
      "2019-07-14 19:50:31.822720: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 19:50:31.836214: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 19:50:31.836420: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de7dfa9f20 executing computations on platform Host. Devices:\n",
      "2019-07-14 19:50:31.836437: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 27514 tid 27514 thread 0 bound to OS proc set 0\n",
      "2019-07-14 19:50:31.839690: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 19:50:31.849768: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 19:50:31.854196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 19:50:31.854388: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5567375dd4b0 executing computations on platform Host. Devices:\n",
      "2019-07-14 19:50:31.854403: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 27516 tid 27516 thread 0 bound to OS proc set 0\n",
      "2019-07-14 19:50:31.855611: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 19:50:31.894019: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 19:50:31.899531: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 19:50:31.899703: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dd6227c180 executing computations on platform Host. Devices:\n",
      "2019-07-14 19:50:31.899718: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 27517 tid 27517 thread 0 bound to OS proc set 0\n",
      "2019-07-14 19:50:31.914549: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 19:50:32.195831: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2019-07-14 19:50:32.214896: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2019-07-14 19:50:32.220570: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2019-07-14 19:50:32.270893: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "OMP: Info #250: KMP_AFFINITY: pid 27515 tid 27561 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 27515 tid 27562 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 27516 tid 27579 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 27516 tid 27580 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 27514 tid 27569 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 27514 tid 27568 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 27517 tid 27590 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 27517 tid 27591 thread 2 bound to OS proc set 2\n",
      "2019-07-14 19:51:27.169684: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-14 19:51:27.192435: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\n",
      "2019-07-14 19:51:27.192820: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b7f5fa4350 executing computations on platform Host. Devices:\n",
      "2019-07-14 19:51:27.192849: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 26830 thread 0 bound to OS proc set 0\n",
      "2019-07-14 19:51:27.193374: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-14 19:51:27.303449: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 27615 thread 1 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 27619 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 27620 thread 3 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 27621 thread 4 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 27616 thread 5 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 27623 thread 7 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 27624 thread 8 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 26830 tid 27622 thread 6 bound to OS proc set 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "results = []\n",
    "grids = []\n",
    "for architecture in architectures : \n",
    "    layers = architecture\n",
    "    print(\"Using architecture: \", layers)\n",
    "    model = KerasClassifier(build_fn=build_DNN, batch_size=10000, epochs=2)\n",
    "    param_grid = dict(epochs=epochs, batch_size=batch_size,\n",
    "                      activation=activation, optimizer=optimizer)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                        n_jobs=-1, pre_dispatch=8)\n",
    "    grid_result = grid.fit(pcTrain, Y_Train)\n",
    "    results.append(grid_result)\n",
    "    grids.append(grid)\n",
    "    print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9581428618771689\n",
      "{'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.9597142886860031\n",
      "{'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n",
      "0.9624285731996809\n",
      "{'activation': 'elu', 'batch_size': 100, 'epochs': 2, 'optimizer': 'SGD'}\n"
     ]
    }
   ],
   "source": [
    "for result in results : \n",
    "    pprint.pprint(result.best_score_)\n",
    "    pprint.pprint(result.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
