{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory of Computational Physics mod. B at Phisics of Data, University of Padova\n",
    "\n",
    "__Authors:__\n",
    "\n",
    "- Valeria Fioroni (University of Padova)\n",
    "- Matteo Guida (University of Padova)\n",
    "- Philipp Zehetner (University of Padova, Ludwig Maximilian University of Munich)\n",
    "\n",
    "__Supervised by:__\n",
    "\n",
    "- Professor Marco Zanetti (University of Padova, CERN)\n",
    "- Professor Stefano Lacaprara (University of Padova, BELLE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Signal\n",
    "\n",
    "At the SuperKEKB paricle accelerator electrons and positrons collide with a center-of-momentum energy clos to the $ \\Upsilon (4S) $ resonance. The cross section for the event $ e^+ e^- \\rightarrow \\Upsilon (4S) $ is \n",
    "\n",
    "$$ \n",
    "\\sigma_{\\Upsilon (4S)} = 1.1 \\, nb\n",
    "$$\n",
    "\n",
    "The $ \\Upsilon (4S) $ decays in one of two (almost) equally probable products: \n",
    "- $ \\Upsilon (4S) \\rightarrow B^+ B^- $\n",
    "- $ \\Upsilon (4S) \\rightarrow B^0 \\bar{B^0} $\n",
    "\n",
    "For this analysis we are interested in the decay channel\n",
    "$$\n",
    "B^0 \\rightarrow \\eta' \\left( \\eta \\left( \\gamma \\gamma \\right) \\pi^+ \\pi^- \\right) K^0_S \\left( \\pi^+ \\pi^- \\right)\n",
    "$$\n",
    "\n",
    "The branching fractions are as follows: \n",
    "\n",
    "- $ \\mathcal{BF} \\left( B^0 \\rightarrow \\eta' K^0 \\right) = 6.6 * 10^{-5} $ \n",
    "- $ \\mathcal{BF} \\left( \\eta' \\rightarrow \\eta \\pi^+ \\pi^- \\right) = 0.43 $\n",
    "- $ \\mathcal{BF} \\left( \\eta \\rightarrow \\gamma \\gamma \\right) = 0.40 $\n",
    "- $ P \\left( K^0_S | K^0 \\right) = 0.5 $\n",
    "- $ \\mathcal{BF} \\left( K^0_S \\rightarrow \\pi^+ \\pi^- \\right) = 0.7 $\n",
    "\n",
    "This leads to a complete branching ratio of \n",
    "$ \\mathcal{BF} = 3.97 * 10^{-6} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Our Data\n",
    "\n",
    "Our Data consists of labelled data, that we used to train and test a _Deep Neural Network_ (DNN) and a _Boosted Decision Tree_ as well as unlabelled data (Data Challenge) in which we are searching for our signal. \n",
    "\n",
    "###  Labelled Data\n",
    "\n",
    "The labelled data consists of various types of background, all corresponding to an integrated luminosity of \n",
    "$ \\mathcal{L} = 800 \\, fb^{-1} $\n",
    "and signal corresponding to __NSIGNAL__ reconstructed signal events with an efficiency of \n",
    "$ \\varepsilon = 0.291 $\n",
    "\n",
    "- $ e^+ e^- \\rightarrow u \\bar{u} $\n",
    "- $ e^+ e^- \\rightarrow d \\bar{d} $\n",
    "- $ e^+ e^- \\rightarrow s \\bar{s} $\n",
    "- $ e^+ e^- \\rightarrow c \\bar{c} $\n",
    "- $ e^+ e^- \\rightarrow \\tau \\bar{\\tau} $\n",
    "- $ e^+ e^- \\rightarrow B^+ B^- $\n",
    "- $ e^+ e^- \\rightarrow B^0 \\bar{B^0} $\n",
    "\n",
    "We summarize the first five types of backgrounds as _Continuum Background_ and the last two types as _Peaking Background_ and thus build a multi classifier with three types of events. \n",
    "\n",
    "Using the complete branching fraction \n",
    "$ \\mathcal{BF} $\n",
    "and the cross section \n",
    "$ \\sigma_{\\Upsilon (4S)} $ \n",
    "we can calculate to expect to have $N$ events with \n",
    "\\begin{equation}\n",
    "N = \\mathcal{L} \\times \\sigma_{\\Upsilon(4S)} \\times \\mathcal{BF} \n",
    "= 800 \\, fb^{-1} \\times 1.1 \\, nb \\times 3.97 \\cdot 10^{-6}\n",
    "= 3494\n",
    "\\end{equation}\n",
    "of which $ \\varepsilon \\times N = 1017 $ should be found by the reconstruction\n",
    "\n",
    "###  Data Challenge\n",
    "\n",
    "The Data Challenge is a simulation corresponding to a total luminosity of \n",
    "$ \\mathcal{L} = 1 \\, ab^{-1} $\n",
    "We can calculate in the same as above, that we should expect a total number of $ N = 4367 $ signal events of which $1271$ are reconstructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pprint\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from root_pandas import read_root\n",
    "from random import seed\n",
    "from random import randint\n",
    "n_seed=1234\n",
    "seed(n_seed)\n",
    "\n",
    "Local_Philipp = True\n",
    "Local_Valeria = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuum: 246329 events\n",
      "Peaking: 2167 events\n",
      "Signal: 218596 events\n",
      "Assuming, that originally 1.6M signal events existed, this corresponds to a reconstruction efficiencey of 13.66225%\n"
     ]
    }
   ],
   "source": [
    "Training = ['B0_Pcms', 'B0_etap_Pcms', 'B0_etap_eta_Pcms', 'B0_etap_eta_gamma0_Pcms', 'B0_etap_eta_gamma1_Pcms', 'B0_etap_pi0_Pcms', 'B0_etap_pi1_Pcms', 'B0_K_S0_Pcms', 'B0_M', 'B0_ErrM', 'B0_SigM', 'B0_K_S0_M', 'B0_K_S0_ErrM', 'B0_K_S0_SigM', 'B0_etap_M', 'B0_etap_ErrM', 'B0_etap_SigM', 'B0_etap_eta_M', 'B0_etap_eta_ErrM', 'B0_etap_eta_SigM', 'B0_deltae', 'B0_mbc', 'B0_TrCSMVA', 'B0_X', 'B0_ErrX', 'B0_Y', 'B0_ErrY', 'B0_Z', 'B0_ErrZ', 'B0_Rho', 'B0_etap_X', 'B0_etap_ErrX', 'B0_etap_Y', 'B0_etap_ErrY', 'B0_etap_Z', 'B0_etap_ErrZ', 'B0_etap_Rho', 'B0_etap_eta_X', 'B0_etap_eta_ErrX', 'B0_etap_eta_Y', 'B0_etap_eta_ErrY', 'B0_etap_eta_Z', 'B0_etap_eta_ErrZ', 'B0_etap_eta_Rho', 'B0_etap_pi0_X', 'B0_etap_pi0_ErrX', 'B0_etap_pi0_Y', 'B0_etap_pi0_ErrY', 'B0_etap_pi0_Z', 'B0_etap_pi0_ErrZ', 'B0_etap_pi0_Rho', 'B0_etap_pi1_X', 'B0_etap_pi1_ErrX', 'B0_etap_pi1_Y', 'B0_etap_pi1_ErrY', 'B0_etap_pi1_Z', 'B0_etap_pi1_ErrZ', 'B0_etap_pi1_Rho', 'B0_K_S0_X', 'B0_K_S0_ErrX', 'B0_K_S0_Y', 'B0_K_S0_ErrY', 'B0_K_S0_Z', 'B0_K_S0_ErrZ', 'B0_K_S0_Rho', 'B0_cosAngleBetweenMomentumAndVertexVector', 'B0_distance', 'B0_significanceOfDistance', 'B0_dr', 'B0_etap_pi0_dr', 'B0_etap_pi1_dr', 'B0_K_S0_dr', 'B0_decayAngle__bo0__bc', 'B0_decayAngle__bo1__bc', 'B0_etap_decayAngle__bo0__bc', 'B0_etap_decayAngle__bo1__bc', 'B0_etap_decayAngle__bo2__bc', 'B0_VtxPvalue', 'B0_etap_VtxPvalue', 'B0_etap_eta_VtxPvalue', 'B0_etap_pi0_VtxPvalue', 'B0_etap_pi1_VtxPvalue', 'B0_K_S0_VtxPvalue']\n",
    "Important = Training + ['B0_isSignal', 'evt_no']\n",
    "\n",
    "if Local_Philipp : \n",
    "    path = '/home/philipp/Desktop/Project/DATA/'\n",
    "    SFiles = glob.glob(os.path.join(path, 'Signal/*.root'))\n",
    "    CFiles = glob.glob(os.path.join(path, 'Continuous/*.root'))\n",
    "    PFiles = glob.glob(os.path.join(path, 'Peaking/*.root'))\n",
    "    DCFiles = glob.glob(os.path.join(path, 'DC/*.root'))\n",
    "    \n",
    "if Local_Valeria : \n",
    "    path = '/home/utente/Scrivania/Progetto LCP-B/DataBelle2_all/'\n",
    "    SFiles = glob.glob(os.path.join(path, 'Signal/*.root'))\n",
    "    CFiles = glob.glob(os.path.join(path, 'Background/Continuum/*.root'))\n",
    "    PFiles = glob.glob(os.path.join(path, 'Background/Peaking/*.root'))\n",
    "    DCFiles = glob.glob(os.path.join(path, 'DataChallenge/*.root'))\n",
    "    \n",
    "Signal = pd.concat((read_root(f, 'B0', columns=Important) for f in SFiles))\n",
    "Signal = Signal[Signal['B0_isSignal']==1].reset_index(drop=True)\n",
    "Continuum_bkg = pd.concat((read_root(f, 'B0', columns=Important) for f in CFiles))\n",
    "Peaking_bkg = pd.concat((read_root(f, 'B0', columns=Important) for f in PFiles))\n",
    "DC = pd.concat((read_root(f, 'B0', columns=Training) for f in DCFiles)).reset_index(drop=True)\n",
    "\n",
    "#Signal: In case of more than 1 candidate per event select the first one occuring\n",
    "Mask_duplicated = Signal.duplicated(subset='evt_no', keep='first')\n",
    "Mask_duplicated=np.logical_not(Mask_duplicated)\n",
    "Signal=Signal[Mask_duplicated]\n",
    "Signal.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Continuum: {} events\".format(Continuum_bkg.shape[0]))\n",
    "print(\"Peaking: {} events\".format(Peaking_bkg.shape[0]))\n",
    "print(\"Signal: {} events \\n\".format(Signal.shape[0]))\n",
    "print(\"Assuming, that originally 1.6M signal events existed, this corresponds to a reconstruction efficiencey of {}%\".format(100*Signal.shape[0]/1600000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing our Data\n",
    "\n",
    "- explain steps in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "n_classes = 3\n",
    "Signal['Type'] = 2\n",
    "Continuum_bkg['Type'] = 1\n",
    "Peaking_bkg['Type'] = 0\n",
    "\n",
    "# Splitting Data and Label into train, test and validation separatly for each class\n",
    "X = Signal.drop('Type',axis=1)\n",
    "Y = Signal['Type']\n",
    "X_train_S, X_test_val_S, Y_train_S, Y_test_val_S = train_test_split(X, Y, train_size=0.5,random_state=randint(10**6,10**9))\n",
    "X = X_test_val_S\n",
    "Y = Y_test_val_S\n",
    "X_validation_S, X_test_S, Y_validation_S, Y_test_S = train_test_split(X, Y, train_size=0.6,random_state=randint(10**6,10**9))\n",
    "del(X_test_val_S)\n",
    "del(Y_test_val_S)\n",
    "X = Continuum_bkg.drop('Type',axis=1)\n",
    "Y = Continuum_bkg['Type']\n",
    "X_train_C, X_test_val_C, Y_train_C, Y_test_val_C = train_test_split(X, Y, train_size=0.5,random_state=randint(10**6,10**9))\n",
    "X = X_test_val_C\n",
    "Y = Y_test_val_C\n",
    "X_validation_C, X_test_C, Y_validation_C, Y_test_C = train_test_split(X, Y, train_size=0.6,random_state=randint(10**6,10**9))\n",
    "del(X_test_val_C)\n",
    "del(Y_test_val_C)\n",
    "X = Peaking_bkg.drop('Type',axis=1)\n",
    "Y = Peaking_bkg['Type']\n",
    "X_train_P, X_test_val_P, Y_train_P, Y_test_val_P = train_test_split(X, Y, train_size=0.5,random_state=randint(10**6,10**9))\n",
    "X = X_test_val_P\n",
    "Y = Y_test_val_P\n",
    "X_validation_P, X_test_P, Y_validation_P, Y_test_P = train_test_split(X, Y, train_size=0.6,random_state=randint(10**6,10**9))\n",
    "del(X_test_val_P)\n",
    "del(Y_test_val_P)\n",
    "\n",
    "# Concatenating the different classes, encoding the label as one-hot vectors and shuffling everything\n",
    "X_Train=np.concatenate((X_train_S, X_train_C, X_train_P), axis=0)\n",
    "X_Validation=np.concatenate((X_validation_S, X_validation_C, X_validation_P), axis=0)\n",
    "X_Test=np.concatenate((X_test_S, X_test_C, X_test_P), axis=0)\n",
    "Y_Train=np.concatenate((Y_train_S, Y_train_C, Y_train_P), axis=0)\n",
    "Y_Validation=np.concatenate((Y_validation_S, Y_validation_C, Y_validation_P), axis=0)\n",
    "Y_Test=np.concatenate((Y_test_S, Y_test_C, Y_test_P), axis=0)\n",
    "Ỳ_Test_not_encoded=Y_Test #used for cut value analysis\n",
    "Y_Train=to_categorical(Y_Train, num_classes=3)\n",
    "Y_Validation=to_categorical(Y_Validation, num_classes=3)\n",
    "Y_Test=to_categorical(Y_Test, num_classes=3)\n",
    "permutation = np.random.permutation(X_Train.shape[0])\n",
    "X_Train = X_Train[permutation]\n",
    "Y_Train = Y_Train[permutation]\n",
    "permutation = np.random.permutation(X_Validation.shape[0])\n",
    "X_Validation = X_Validation[permutation]\n",
    "Y_Validation = Y_Validation[permutation]\n",
    "permutation = np.random.permutation(X_Test.shape[0])\n",
    "X_Test = X_Test[permutation]\n",
    "Y_Test = Y_Test[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write something about the 5 sigma mass cut and why we aren't doing it\n",
    "It's something we tried, so worth mentioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write something about scaling and why it is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "pcTrain = ss.fit_transform(X_Train)\n",
    "pcTest = ss.transform(X_Test)\n",
    "pcValidation = ss.transform(X_Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write something about PCA, and why we are not doing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Neural Network\n",
    "\n",
    "explain what neural network we are using, how we have been training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_148 (Dropout)        (None, 83)                0         \n",
      "_________________________________________________________________\n",
      "dense_662 (Dense)            (None, 50)                4200      \n",
      "_________________________________________________________________\n",
      "dense_663 (Dense)            (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_664 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_665 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_666 (Dense)            (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 24,603\n",
      "Trainable params: 24,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "Model = load_model('Models/LastTry/Best_Model.h5')\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about the performance of our model \n",
    "\n",
    "- Model.evaluate with Test sample\n",
    "- ROCK Curve\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBOOST\n",
    "\n",
    "- Same story here as in the last chapter\n",
    "- Advantages/Disadvantages compared to the DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deciding on the Cut Value of our Network\n",
    "\n",
    "- all necessary steps and explanations for the cut value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Data Challenge\n",
    "\n",
    "Here we con showcase the perfect performance of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion or additional comments (optional)\n",
    "\n",
    "If you think somthing is missing feel free to add it. \n",
    "With 6 topics we can give 2 (non consecutive) topics for everybody. \n",
    "Having a little break between your two topics maybe helps a bit if something didn't go as expected. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
